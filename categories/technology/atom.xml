<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Technology | David Xia]]></title>
  <link href="https://www.davidxia.com/categories/technology/atom.xml" rel="self"/>
  <link href="https://www.davidxia.com/"/>
  <updated>2021-08-07T12:59:29-04:00</updated>
  <id>https://www.davidxia.com/</id>
  <author>
    <name><![CDATA[David Xia]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[What I Recently Learned About Docker Networking and Debugging Networking Issues in General]]></title>
    <link href="https://www.davidxia.com/2021/05/what-i-recently-learned-about-docker-networking-and-debugging-network-issues-in-general/"/>
    <updated>2021-05-02T16:39:02-07:00</updated>
    <id>https://www.davidxia.com/2021/05/what-i-recently-learned-about-docker-networking-and-debugging-network-issues-in-general</id>
    <content type="html"><![CDATA[<p>This is a story about how debugged a confounding local development environment issue, what I
learned about Docker in the process, and the generally applicable debugging strategies and
techniques that helped me ultimately solve it. Skip to the end if you only want to read the
debugging strategies and techniques. The overall story, however, will illustrate how they applied
in this specific case.</p>

<h2>Problem Statement and Use Case</h2>

<p>A data infrastructure team at work provides a tool for starting a data pipeline job from a local
development environment. Let&rsquo;s call this tool <code>foo</code>. This tool depends on <code>gcloud</code> and <code>docker</code>.
It creates a user-defined Docker network, runs a utility container called <code>bar</code> connected to that
network, and then runs another container called qux that talks to bar to retrieve Oauth tokens
from Google Cloud Platform (GCP).</p>

<p>Most developers run <code>foo</code> on their local workstations, e.g. Macbooks. But I have the newer
Macbook with the Apple M1 ARM-based chip. <a href="https://docs.docker.com/docker-for-mac/install/">Docker Desktop on Mac</a> support for M1s was
relatively recent. I didn&rsquo;t want deal with Docker weirdness. I also didn&rsquo;t have a lot of free
disk space on my 256GB Macbook and thus didn&rsquo;t feel like clogging up my drive with lots of Java,
Scala, and Docker gunk.</p>

<p>So I tried running <code>foo</code> on a GCE VM configured by our <a href="https://puppet.com/">Puppet</a> configuration files. I ran <code>foo</code>,
I got this error.</p>

<!-- more -->


<h2>Error #1: inter-container networking failed between containers attached to user-defined Docker networks</h2>

<pre><code>dxia@my-host$ foo --verbose run -f data-info.yaml -w DumpKubernetesContainerImagesJob -p 2021-04-26 -r my-project/target/image-name
DEBUG:verify: Docker network `foo-network` already exists
DEBUG:verify: bar container found.
INFO:run: starting workflow DumpKubernetesContainerImagesJob ...
ERROR: (gcloud.auth.activate-service-account) [Errno 110] Connection timed out
This may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.
Traceback (most recent call last):
  File "/usr/local/bin/activate-google-application-credentials", line 19, in &lt;module&gt;
    'auth', 'activate-service-account', '--key-file', json_path])
  File "/usr/lib/python3.6/subprocess.py", line 311, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['gcloud', 'auth', 'activate-service-account', '--key-file', '/etc/_foo/gcp-sa-key.json']' returned non-zero exit status 1.
ERROR:foo:

  RAN: /usr/bin/docker run -it -v /home/dxia/my-project/_foo:/etc/_foo:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/site-packages/oauth2client/__init__.py:ro --net foo-network -e FOO_COMPONENT_ID=my-project -e FOO_WORKFLOW_ID=DumpKubernetesContainerImagesJob -e FOO_PARAMETER=2021-04-26 -e FOO_DOCKER_IMAGE=my-project:20210426T211411-2b5452d -e 'FOO_DOCKER_ARGS=wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26' -e FOO_EXECUTION_ID=foorun-2e30c385-2f89-494c-bc0e-97b3eff316d5 -e FOO_TRIGGER_ID=foo-942f155b-49eb-4af8-a6e4-3adf6f72577b -e FOO_TRIGGER_TYPE=foo -e FOO_ENVIRONMENT=foo -e FOO_LOGGING=text -e GOOGLE_APPLICATION_CREDENTIALS=/etc/_foo/gcp-sa-key.json -e FOO_SERVICE_ACCOUNT=dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com gcr.io/xpn-1/my-project:20210426T211411-2b5452d wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26

  STDOUT:


  STDERR:
Traceback (most recent call last):
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/foo.py", line 304, in main
    args.func(args)
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/foo.py", line 269, in _run
    args.declarative_infra,
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/run.py", line 641, in run_workflow
    declarative_infra,
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/run.py", line 161, in _run_workflow
    p.wait()
  File "/home/dxia/my-project//lib/python3.6/site-packages/sh.py", line 841, in wait
    self.handle_command_exit_code(exit_code)
  File "/home/dxia/my-project//lib/python3.6/site-packages/sh.py", line 865, in handle_command_exit_code
    raise exc
sh.ErrorReturnCode_1:

  RAN: /usr/bin/docker run -it -v /home/dxia/my-project/_foo:/etc/_foo:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/site-packages/oauth2client/__init__.py:ro --net foo-network -e FOO_COMPONENT_ID=my-project -e FOO_WORKFLOW_ID=DumpKubernetesContainerImagesJob -e FOO_PARAMETER=2021-04-26 -e FOO_DOCKER_IMAGE=my-project:20210426T211411-2b5452d -e 'FOO_DOCKER_ARGS=wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26' -e FOO_EXECUTION_ID=foorun-2e30c385-2f89-494c-bc0e-97b3eff316d5 -e FOO_TRIGGER_ID=foo-942f155b-49eb-4af8-a6e4-3adf6f72577b -e FOO_TRIGGER_TYPE=foo -e FOO_ENVIRONMENT=foo -e FOO_LOGGING=text -e GOOGLE_APPLICATION_CREDENTIALS=/etc/_foo/gcp-sa-key.json -e FOO_SERVICE_ACCOUNT=dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com gcr.io/xpn-1/my-project:20210426T211411-2b5452d wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26

  STDOUT:


  STDERR:
</code></pre>

<p>The HTTP connection timed out. First I checked whether the container started by <code>foo</code> can make a TCP
connection to the bar container. I ran <code>foo --verbose run -f data-info.yaml -w
DumpKubernetesContainerImagesJob -p 2021-04-26 -r my-project/target/image-name</code>
again and did the following in another terminal window.</p>

<p><a href="https://man7.org/linux/man-pages/man1/nsenter.1.html"><code>nsenter</code></a> is a cool tool that allows you to run programs in different Linux namespaces.
It&rsquo;s very useful when you can&rsquo;t get an executable shell into a container with commands like
<code>docker exec -it ... bash</code>. This can happen when the container doesn&rsquo;t even include any shells
and just has the binary executable for instance.</p>

<pre><code>dxia@my-host:~$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED             STATUS              PORTS                     NAMES
a0e872188831        my-project:20210426T211411-2b5452d         "/usr/local/bin/edgeâ€¦"   2 seconds ago       Up 1 second                                   relaxed_pike
4dda670a2ee1        foo/bar:latest                                   "./bar"               2 hours ago         Up 2 hours          0.0.0.0:80-&gt;80/tcp        bar

dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} a0e872188831)  nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) : Connection timed out
</code></pre>

<p>So the HTTP connection timeout was caused by an error lower down on the networking stack: an
inability to establish a TCP connection. A TCP connection from the host to bar worked though.</p>

<pre><code>dxia@my-host:~$ nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) open
</code></pre>

<p>When I see a networking issue like this, I know there might be some misconfigured firewall rule
blocking IP packets. I listed all the firewall rules. The ones in the filter table&rsquo;s <code>FORWARD</code>
chain caught my attention.</p>

<pre><code>dxia@my-host:~$ sudo iptables --list FORWARD --verbose --numeric --line-numbers --table filter
Chain FORWARD (policy DROP 38 packets, 2280 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1     6204  492K DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
2     6204  492K DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
3     3080  323K ACCEPT     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
4        1    60 DOCKER     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0
5     3085  167K ACCEPT     all  --  corp0 !corp0  0.0.0.0/0            0.0.0.0/0
6        0     0 ACCEPT     all  --  corp0 corp0  0.0.0.0/0            0.0.0.0/0
7      264 17722 DOCKER     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
8     7382   17M ACCEPT     all  --  br-8ce7e363e4f9 !br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>I disabled the GCE VM&rsquo;s cronned Puppet run and then ran <code>sudo systemctl restart docker</code>. I ran
bar and a test nginx1 container connected to <code>foo-network</code>.</p>

<pre><code>dxia@my-host:~$ docker run --rm -d -v ~/.config/gcloud/:/.config/gcloud --name bar --net foo-network --ip 172.20.0.127 -p 80:80 foo/bar:latest
3f9cc17b3f71e7056fd8072449afa78eb9a6a166ac091d751b69545ead0438b1

dxia@my-host:~$ docker run --net foo-network --name nginx1 -d -p 8080:80 nginx:latest
1b0b2b981f9389a989aa8f60a141b5e9a18ba5582141b6668c9078b6312dcfaf

dxia@my-host:~$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED              STATUS              PORTS                     NAMES
1b0b2b981f93        nginx:latest                                                             "/docker-entrypoint.â€¦"   5 seconds ago        Up 3 seconds        0.0.0.0:8080-&gt;80/tcp      nginx1
3f9cc17b3f71        foo/bar:latest                                   "./bar"               About a minute ago   Up 59 seconds       0.0.0.0:80-&gt;80/tcp        bar
</code></pre>

<p>Now a TCP connection from the nginx container to bar succeeded.</p>

<pre><code>dxia@my-host:~$ sudo nsenter --net=$(docker inspect --format {{.NetworkSettings.SandboxKey}} nginx1) nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) open
</code></pre>

<p>I checked iptables rules again and saw two additional rules (7 and 8) in the filter table&rsquo;s
<code>FORWARD</code> chain. Rule 8 allowed IP packets coming in from the <code>br-8ce7e363e4f9</code> network interface
(in this case a <a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/#bridge">Linux bridge</a>) and leaving through the same interface.</p>

<pre><code>dxia@my-host:~$ sudo iptables --list FORWARD --verbose --numeric --line-numbers --table filter
Chain FORWARD (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1        0     0 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
2        0     0 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
3        0     0 ACCEPT     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
4        0     0 DOCKER     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0
5        0     0 ACCEPT     all  --  corp0 !corp0  0.0.0.0/0            0.0.0.0/0
6        0     0 ACCEPT     all  --  corp0 corp0  0.0.0.0/0            0.0.0.0/0
7        0     0 ACCEPT     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
8        0     0 ACCEPT     all  --  br-8ce7e363e4f9 br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
9      264 17722 DOCKER     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
10    7382   17M ACCEPT     all  --  br-8ce7e363e4f9 !br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>When I re-ran Puppet rules 7 and 8 were deleted and containers on the <code>foo-network</code> were again
unable to establish a TCP connection. I added rule 8 manually and confirmed this is the rule
causing my error above.</p>

<pre><code>dxia@my-host:~$ sudo iptables --table filter --append FORWARD --in-interface br-8ce7e363e4f9 --out-interface br-8ce7e363e4f9 --source 0.0.0.0/0 --destination 0.0.0.0/0 --jump ACCEPT

dxia@my-host:~$ sudo iptables --list FORWARD --verbose --numeric --line-numbers --table filter
Chain FORWARD (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1    23526 1377K DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
2    23526 1377K DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
3    11728  755K ACCEPT     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
4        1    60 DOCKER     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0
5    11737  617K ACCEPT     all  --  corp0 !corp0  0.0.0.0/0            0.0.0.0/0
6        0     0 ACCEPT     all  --  corp0 corp0  0.0.0.0/0            0.0.0.0/0
7      182 12970 DOCKER     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
8       39  2748 ACCEPT     all  --  br-8ce7e363e4f9 !br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
9        0     0 ACCEPT     all  --  br-8ce7e363e4f9 br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0

dxia@my-host:~$ sudo nsenter --net=$(docker inspect --format {{.NetworkSettings.SandboxKey}} nginx1) nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) open
</code></pre>

<p>Now running <code>foo</code> gave a different error.</p>

<h2>Error #2: DNS queries for external records from bar failed</h2>

<pre><code>dxia@my-host$ foo run -f data-info.yaml -w DumpKubernetesContainerImagesJob -p 2021-04-26 -r my-project/target/image-name
INFO:run: starting workflow DumpKubernetesContainerImagesJob ...
ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: Invalid response 500.
Please run:

  $ gcloud auth login

to obtain new credentials.

If you have already logged in with a different account:

    $ gcloud config set account ACCOUNT

to select an already authenticated account to use.
Traceback (most recent call last):
  File "/usr/local/bin/activate-google-application-credentials", line 19, in &lt;module&gt;
    'auth', 'activate-service-account', '--key-file', json_path])
  File "/usr/lib/python3.6/subprocess.py", line 311, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['gcloud', 'auth', 'activate-service-account', '--key-file', '/etc/_foo/gcp-sa-key.json']' returned non-zero exit status 1.
ERROR:foo: non-zero exit code (1) from `/usr/bin/docker run -it -v /home/dxia/my-project/_foo:/etc/_foo:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/site-packages/oauth2client/__init__.py:ro --net foo-network -e FOO_COMPONENT_ID=my-project -e FOO_WORKFLOW_ID=DumpKubernetesContainerImagesJob -e FOO_PARAMETER=2021-04-02 -e FOO_DOCKER_IMAGE=my-project:20210422T065801-2b5452d -e 'FOO_DOCKER_ARGS=wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-02' -e FOO_EXECUTION_ID=foorun-3feaee45-35e4-4c01-9430-86de52eb2db1 -e FOO_TRIGGER_ID=foo-f721de7f-edf9-4bb3-8cdf-1e9bbcec5035 -e FOO_TRIGGER_TYPE=foo -e FOO_ENVIRONMENT=foo -e FOO_LOGGING=text -e GOOGLE_APPLICATION_CREDENTIALS=/etc/_foo/gcp-sa-key.json -e FOO_SERVICE_ACCOUNT=dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com gcr.io/xpn-1/my-project:20210422T065801-2b5452d wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-02`
</code></pre>

<p>The only background knowledge we need to know here is that the qux container is sending a Google
Service Account (GSA) JSON credential with <code>"token_uri": "http://172.20.0.127:80/token"</code>. Bar
then uses that token for further GCP API requests. So bar needs to query DNS for
accounts.google.com. Bar container logs show that it cannot lookup the DNS A record for
accounts.google.com by querying <code>127.0.0.11:53</code>.</p>

<pre><code>dxia@my-host:~$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED             STATUS              PORTS                     NAMES
1b0b2b981f93        nginx:latest                                                             "/docker-entrypoint.â€¦"   9 hours ago         Up 9 hours          0.0.0.0:8080-&gt;80/tcp      nginx1
3f9cc17b3f71        foo/bar:latest                                   "./bar"               9 hours ago         Up 9 hours          0.0.0.0:80-&gt;80/tcp        bar

dxia@my-host:~$ docker logs --follow bar
2021/04/22 05:54:19 bar started
2021/04/22 06:59:13 Received JWT assertion: [REDACTED base-64 string]
2021/04/22 06:59:13 Servive account name:  projects/-/serviceAccounts/dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com
2021/04/22 06:59:28 Post https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com:generateAccessToken?alt=json&amp;prettyPrint=false: Post https://accounts.google.com/o/oauth2/token: dial tcp: lookup accounts.google.com on 127.0.0.11:53: read udp 127.0.0.1:46920-&gt;127.0.0.11:53: i/o timeout
2021/04/22 06:59:28 Failed to create new token for dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com
</code></pre>

<p>I wondered why bar was querying <code>127.0.0.11</code> for DNS. It turns out this is another loopback
address. In fact, all of <code>127.0.0.0/8</code> is loopback according to <a href="https://tools.ietf.org/html/rfc6890">RFC-6890</a>. I guess Docker
containers that are attached to user-defined Docker networks are configured by default to use
<code>127.0.0.11</code> in their <code>/etc/resolv.conf</code>.</p>

<pre><code>dxia@my-host$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED             STATUS              PORTS                     NAMES
1b0b2b981f93        nginx:latest                                                             "/docker-entrypoint.â€¦"   9 hours ago         Up 9 hours          0.0.0.0:8080-&gt;80/tcp      nginx1
3f9cc17b3f71        foo/bar:latest                                   "./bar"               9 hours ago         Up 9 hours          0.0.0.0:80-&gt;80/tcp        bar

dxia@my-host$ docker exec -it nginx1 /bin/sh -c "cat /etc/resolv.conf"

search corp.net
nameserver 127.0.0.11
options attempts:1 timeout:5 ndots:0
</code></pre>

<p>Why were these Docker containers configured to query for DNS records on <code>127.0.0.11</code>? It turned
out after some Googling that</p>

<blockquote><p>By default, a container inherits the DNS settings of the host, as defined in the /etc/resolv.conf
configuration file. Containers that use the default bridge network get a copy of this file,
whereas containers that use a custom network use Dockerâ€™s embedded DNS server, which forwards
external DNS lookups to the DNS servers configured on the host.</p></blockquote>

<p>&mdash; <a href="https://docs.docker.com/config/containers/container-networking/">https://docs.docker.com/config/containers/container-networking/</a></p>

<p>Now I wondered if Docker&rsquo;s embedded DNS server is actually running. After some more Googling, I
realized that each container also had its own set of firewall rules. So I listed bar&rsquo;s nat
table&rsquo;s <code>DOCKER_OUTPUT</code> chain&rsquo;s rules. These two rules showed that the destination port is
changed for TCP packets bound for 127.0.0.11:53 to 37619. UDP packets have their port changed to
58552.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) sudo iptables --list DOCKER_OUTPUT --verbose --numeric --line-numbers --table nat

Chain DOCKER_OUTPUT (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            127.0.0.11           tcp dpt:53 to:127.0.0.11:37619
    0     0 DNAT       udp  --  *      *       0.0.0.0/0            127.0.0.11           udp dpt:53 to:127.0.0.11:58552
</code></pre>

<p>Whatever&rsquo;s listening on those ports was accepting TCP and UDP connections.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) nc 127.0.0.11 58552 -nvzu -w 5
(UNKNOWN) [127.0.0.11] 58552 (?) open
dxia@my-host$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) nc 127.0.0.11 37619 -nvz -w 5
(UNKNOWN) [127.0.0.11] 37619 (?) open
</code></pre>

<p>But there was no DNS reply from either.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) dig @127.0.0.11 -p 58552 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 -p 58552 accounts.google.com
; (1 server found)
;; global options: +cmd
;; connection timed out; no servers could be reached

dxia@my-host$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) dig @127.0.0.11 -p 37619 accounts.google.com +tcp

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 -p 37619 accounts.google.com +tcp
; (1 server found)
;; global options: +cmd
;; connection timed out; no servers could be reached
</code></pre>

<p>Docker daemon was listening for DNS queries at that IP and port from within bar.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -p -t $(docker inspect --format {{.State.Pid}} bar) ss -utnlp
Netid        State          Recv-Q         Send-Q                    Local Address:Port                    Peer Address:Port
udp          UNCONN         0              0                            127.0.0.11:58552                        0.0.0.0:*             users:(("dockerd",pid=10984,fd=38))
tcp          LISTEN         0              128                          127.0.0.11:37619                        0.0.0.0:*             users:(("dockerd",pid=10984,fd=40))
tcp          LISTEN         0              128                                   *:80                                 *:*             users:(("bar",pid=12150,fd=3))
</code></pre>

<p>After enabling <code>log-level": "debug"</code> in <code>/etc/docker/daemon.json</code> and reloading the configuration
file, I saw that the daemon was trying to forward the DNS query to 10.99.0.1. This was the IP of
the <code>corp0</code> bridge network interface which we create instead of the default <code>docker0</code> bridge
network. I saw there was an IO timeout when the daemon was waiting for the DNS reply.</p>

<pre><code>dxia@my-host$ sudo journalctl --follow -u docker
-- Logs begin at Tue 2019-11-05 18:17:27 UTC. --
Apr 22 15:43:12 my-host.corp.net dockerd[10984]: time="2021-04-22T15:43:12.496979903Z" level=debug msg="[resolver] read from DNS server failed, read udp 172.20.0.127:37928-&gt;10.99.0.1:53: i/o timeout"
Apr 22 15:43:13 my-host.corp.net dockerd[10984]: time="2021-04-22T15:43:13.496539033Z" level=debug msg="Name To resolve: accounts.google.com."
Apr 22 15:43:13 my-host.corp.net dockerd[10984]: time="2021-04-22T15:43:13.496958664Z" level=debug msg="[resolver] query accounts.google.com. (A) from 172.20.0.127:51642, forwarding to udp:10.99.0.1"
</code></pre>

<p>We set dockerd&rsquo;s upstream DNS server as 10.99.0.1 because we have unbound running as a DNS
proxy/cache on the host. We configured it to bind on the bridge interface so Docker containers
can hit the host-local unbound instance by routing DNS requests to corp0.</p>

<p>So why can&rsquo;t the daemon forward IP packets from 172.20.0.127:37928 to 10.99.0.1:53? It seemed
like UDP packets sent from bar were able to reach 10.99.0.1:53, but DNS requests failed. I also
knew DNS requests from the host to 10.99.0.1:53 worked.</p>

<pre><code>dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) nc 10.99.0.1 53 -nvzu -w 5
(UNKNOWN) [10.99.0.1] 53 (domain) open

dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) dig @10.99.0.1 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @10.99.0.1 accounts.google.com
; (1 server found)
;; global options: +cmd
;; connection timed out; no servers could be reached

dxia@my-host:~$ dig @10.99.0.1 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @10.99.0.1 accounts.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 39308
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;accounts.google.com.       IN  A

;; ANSWER SECTION:
accounts.google.com.    108 IN  A   142.250.31.84

;; Query time: 1 msec
;; SERVER: 10.99.0.1#53(10.99.0.1)
;; WHEN: Mon Apr 26 23:58:28 UTC 2021
;; MSG SIZE  rcvd: 64
</code></pre>

<p>My hypothesis at this point was that Docker&rsquo;s embedded DNS server wasn&rsquo;t working in some way.
After exploring this for a while with no luck, I questioned my assumption that UDP packets from
172.20.0.127:37928 were able to reach 10.99.0.1:53. I realized TCP packets from
172.20.0.127:37928 were not able to reach 10.99.0.1:53.</p>

<pre><code>dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) nc 10.99.0.1 53 -nvz -w 5
(UNKNOWN) [10.99.0.1] 53 (domain) : Connection timed out
</code></pre>

<p>So why were UDP packets able to? Isn&rsquo;t UDP a fire-and-forget protocol? How can <code>nc</code> even tell if
an IP and port is listening for UDP packets at all? It was good that I backtracked and questioned
my assumption because it turns out that one <a href="https://serverfault.com/questions/416205/testing-udp-port-connectivity/416269#416269">cannot distinguish between an open UDP port and
dropped packets en route to that port</a>.</p>

<p>So it must be another networking issue which means there must be another firewall rule that&rsquo;s
blocking packets from the bar container to 10.99.0.1. After a while of looking, I realized the
filter table&rsquo;s <code>INPUT</code> chain&rsquo;s default policy was <code>DROP</code> and that there was no rule that matched
packets coming in from the <code>br-8ce7e363e4f9</code> interface.</p>

<pre><code>dxia@my-host:~$ sudo iptables --list INPUT --verbose --numeric --line-numbers --table filter
Chain INPUT (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1     434M  345G            all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth0 input */
2        0     0            all  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth1 input */
3     7080  283K DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 drop invalid */ ctstate INVALID
4    1907M  568G ipthrouput  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 track forward */
5     987M  414G ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 001 accept established */ ctstate RELATED,ESTABLISHED
6     763M   95G ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0            /* 002 allow local */
7        0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 003 accept ipsec */ policy match dir in pol ipsec
8        1    28 ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 004 allow icmp */ icmptype 8 limit: avg 10/sec burst 5
9        0     0 DROP       icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 005 drop icmp */
10       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 006 block JMX on service net */
11       0     0 REJECT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 020 deny direct access to JMX port on helios nodes */ reject-with icmp-port-unreachable
12       0     0 DROP       all  --  eth0   *       10.48.64.0/22        0.0.0.0/0            /* 08 drop traffic from osxenv 10.48.64.0/22 */
13       0     0 DROP       all  --  eth0   *       10.97.16.0/21        0.0.0.0/0            /* 08 drop traffic from osxenv 10.97.16.0/21 */
14       0     0 DROP       all  --  eth0   *       172.24.32.0/22       0.0.0.0/0            /* 08 drop traffic from windowsbuildagentsenv 172.24.32.0/22 */
15       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from buildagent machines */ match-set buildagent src
16       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from paymentbamboo machines */ match-set paymentbamboo src
17   18168 1094K ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow service nets */ match-set service_nets src
18     134  8496 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow tech offices */ match-set tech_offices src
19       0     0 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow testenv */ match-set testenv src
20       0     0 ACCEPT     all  --  eth0   *       130.211.0.0/22       0.0.0.0/0            /* 10 Google networks for 130.211.0.0/22 */
21       0     0 ACCEPT     all  --  eth0   *       35.191.0.0/16        0.0.0.0/0            /* 10 Google networks for 35.191.0.0/16 */
22       0     0 LOG        tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth0: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth0: "
23       0     0 LOG        tcp  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth1: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth1: "
24       0     0 ACCEPT     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all docker0 traffic */
25    158M   59G ACCEPT     all  --  corp0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all corp0 traffic */
26       0     0 ACCEPT     tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 500 allow ssh on service net */ ctstate NEW recent: SET name: DEFAULT side: source mask: 255.255.255.255
27       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 501 limit ssh on service net */ ctstate NEW recent: UPDATE seconds: 180 hit_count: 20 name: DEFAULT side: source mask: 255.255.255.255
</code></pre>

<p>So I added a matching rule that accepted those packets manually.</p>

<pre><code>sudo iptables --table filter --append INPUT --in-interface br-8ce7e363e4f9 --source 0.0.0.0/0 --destination 0.0.0.0/0 --jump ACCEPT

dxia@my-host:~$ sudo iptables --list INPUT --verbose --numeric --line-numbers --table filter
Chain INPUT (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1     434M  345G            all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth0 input */
2        0     0            all  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth1 input */
3     7080  283K DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 drop invalid */ ctstate INVALID
4    1908M  568G ipthrouput  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 track forward */
5     987M  414G ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 001 accept established */ ctstate RELATED,ESTABLISHED
6     763M   95G ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0            /* 002 allow local */
7        0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 003 accept ipsec */ policy match dir in pol ipsec
8        1    28 ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 004 allow icmp */ icmptype 8 limit: avg 10/sec burst 5
9        0     0 DROP       icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 005 drop icmp */
10       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 006 block JMX on service net */
11       0     0 REJECT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 020 deny direct access to JMX port on helios nodes */ reject-with icmp-port-unreachable
12       0     0 DROP       all  --  eth0   *       10.48.64.0/22        0.0.0.0/0            /* 08 drop traffic from osxenv 10.48.64.0/22 */
13       0     0 DROP       all  --  eth0   *       10.97.16.0/21        0.0.0.0/0            /* 08 drop traffic from osxenv 10.97.16.0/21 */
14       0     0 DROP       all  --  eth0   *       172.24.32.0/22       0.0.0.0/0            /* 08 drop traffic from windowsbuildagentsenv 172.24.32.0/22 */
15       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from buildagent machines */ match-set buildagent src
16       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from paymentbamboo machines */ match-set paymentbamboo src
17   18168 1094K ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow service nets */ match-set service_nets src
18     134  8496 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow tech offices */ match-set tech_offices src
19       0     0 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow testenv */ match-set testenv src
20       0     0 ACCEPT     all  --  eth0   *       130.211.0.0/22       0.0.0.0/0            /* 10 Google networks for 130.211.0.0/22 */
21       0     0 ACCEPT     all  --  eth0   *       35.191.0.0/16        0.0.0.0/0            /* 10 Google networks for 35.191.0.0/16 */
22       0     0 LOG        tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth0: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth0: "
23       0     0 LOG        tcp  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth1: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth1: "
24       0     0 ACCEPT     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all docker0 traffic */
25    158M   59G ACCEPT     all  --  corp0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all corp0 traffic */
26       0     0 ACCEPT     tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 500 allow ssh on service net */ ctstate NEW recent: SET name: DEFAULT side: source mask: 255.255.255.255
27       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 501 limit ssh on service net */ ctstate NEW recent: UPDATE seconds: 180 hit_count: 20 name: DEFAULT side: source mask: 255.255.255.255
28       0     0 ACCEPT     all  --  br-8ce7e363e4f9 *       0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>I retried querying for accounts.google.com, and I got a DNS reply!</p>

<pre><code>dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) dig @127.0.0.11 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 accounts.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: REFUSED, id: 9592
;; flags: qr rd ad; QUERY: 0, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0
;; WARNING: recursion requested but not available

;; Query time: 0 msec
;; SERVER: 127.0.0.11#53(127.0.0.11)
;; WHEN: Tue Apr 27 00:07:13 UTC 2021
;; MSG SIZE  rcvd: 12
</code></pre>

<p>But&hellip; there&rsquo;s no A records? Docker daemon logs stated that the upstream local unbound DNS server
did not return any A records.</p>

<h2>Error #3: unbound refused to reply to DNS queries from a private IP range used by the Docker network we&rsquo;re using</h2>

<pre><code>Apr 27 00:07:13 my-host.corp.net dockerd[32659]: time="2021-04-27T00:07:13.864160829Z" level=debug msg="Name To resolve: accounts.google.com."
Apr 27 00:07:13 my-host.corp.net dockerd[32659]: time="2021-04-27T00:07:13.864325564Z" level=debug msg="[resolver] query accounts.google.com. (A) from 172.20.0.127:57576, forwarding to udp:10.99.0.1"
Apr 27 00:07:13 my-host.corp.net dockerd[32659]: time="2021-04-27T00:07:13.864537556Z" level=debug msg="[resolver] external DNS udp:10.99.0.1 did not return any A records for \"accounts.google.com.\""
</code></pre>

<p>Hm, I noticed the status in the empty DNS reply is <code>REFUSED</code>. I recalled that <a href="https://linux.die.net/man/5/unbound.conf">unbound supports
configuring which DNS queries it will reply to based on originating interface and
IP</a>.</p>

<pre><code>dxia@my-host:~$ grep access-control /etc/unbound/unbound.conf
    access-control: 127.0.0.1 allow
    access-control: 10.99.0.0/24 allow
    access-control: 10.174.18.90 allow
</code></pre>

<p>Bingo! There&rsquo;s no <code>access-control</code> entry that allowed DNS queries from 172.20.0.127. I added
<code>access-control: 172.16.0.0/12 allow</code> (since all of 172.16.0.0/12 is private IPv4 address space
according to <a href="https://tools.ietf.org/html/rfc1918">RFC-1918</a>) and reloaded unbound. Now it worked!</p>

<pre><code>dxia@my-host:~$ sudo systemctl reload unbound
dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format {{.State.Pid}} bar) dig @127.0.0.11 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 accounts.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 36432
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;accounts.google.com.       IN  A

;; ANSWER SECTION:
accounts.google.com.    143 IN  A   74.125.202.84

;; Query time: 2 msec
;; SERVER: 127.0.0.11#53(127.0.0.11)
;; WHEN: Tue Apr 27 00:11:41 UTC 2021
;; MSG SIZE  rcvd: 64
</code></pre>

<p>Docker daemon logs showed the following.</p>

<pre><code>Apr 27 00:11:41 my-host.corp.net dockerd[32659]: time="2021-04-27T00:11:41.957934764Z" level=debug msg="Name To resolve: accounts.google.com."
Apr 27 00:11:41 my-host.corp.net dockerd[32659]: time="2021-04-27T00:11:41.958087666Z" level=debug msg="[resolver] query accounts.google.com. (A) from 172.20.0.127:33346, forwarding to udp:10.99.0.1"
Apr 27 00:11:41 my-host.corp.net dockerd[32659]: time="2021-04-27T00:11:41.960007990Z" level=debug msg="[resolver] received A record \"74.125.202.84\" for \"accounts.google.com.\" from udp:10.99.0.1"
</code></pre>

<h2>General Debugging Strategies and Techniques I Used</h2>

<p>Here are the general debugging strategies I used and reinforced for myself.</p>

<ul>
<li>When network requests fail, go down one layer on the stack to identify on exactly which layer it
fails. I.e. find out which protocol is responsible for the failure: HTTP, TCP, IP?</li>
<li>Try to reproduce the error by running the most direct and minimal command that simulates my actual
failure. In this case, an HTTP request made by <code>gcloud</code> was failing. I translated that into an <code>nc</code> command
that simulated the establishment of the TCP connection between containers. Or a DNS query from
bar was failing. I translated that into a <code>dig</code> command. And in all these cases, the origin of
these IP packets mattered. So knowing how to use <code>nsenter</code> to enter a network namespace and create
IP packets that originate from the same container was useful. <code>nsenter</code> is essential when debugging
containers that don&rsquo;t have any tools installed in them. The bar image only contains one
go-compiled executable. There&rsquo;s no other tools I can use in there.</li>
<li>I encountered three errors in this case. Tackle one error at a time, and don&rsquo;t give up.</li>
<li>Be scientific. Have a working hypothesis at each step for which you collect evidence that either
supports or refutes it.</li>
<li>If you get stuck, go back and question your previous assumptions or conclusions. Are there other
tests you can run that can confirm or disprove what you thought was true?</li>
</ul>


<h2>Patches</h2>

<p>Error #1: I created a patch that makes our Puppet installation ignore rules created by Docker
networks in the filter table&rsquo;s FORWARD chain.</p>

<p>Error #2: Unfortunately, I don&rsquo;t think there&rsquo;s a good solution to this other than disabling our
GCE VM&rsquo;s periodic Puppet runs and manually adding a rule to allow packets from the new interface.
The chain&rsquo;s default policy is <code>DROP</code>, and interface names are dynamic.</p>

<p>Error #3: I made a patch that makes unbound reply to DNS queries with source IPs of in the range
<code>172.16.0.0/12</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Kubernetes Routes IP Packets to Services' Cluster IPs]]></title>
    <link href="https://www.davidxia.com/2021/01/how-kubernetes-routes-ip-packets-to-services-cluster-ips/"/>
    <updated>2021-01-27T13:22:02-05:00</updated>
    <id>https://www.davidxia.com/2021/01/how-kubernetes-routes-ip-packets-to-services-cluster-ips</id>
    <content type="html"><![CDATA[<p>I recently observed DNS resolution errors on a large Kubernetes (K8s) cluster. This behavior was
only happening on 0.1% of K8s nodes. But the fact that this behavior wasn&rsquo;t self-healing and
crippled tenant workloads in addition to my penchant to chase rabbits down holes meant I
wasn&rsquo;t going to let it go. I emerged learning how K8s Services&#8217; Cluster IP feature actually works.
Explaining this feature and my particular problem and speculative fix is the goal of this post.</p>

<h2>The Problem</h2>

<p>The large K8s cluster is actually a Google Kubernetes Engine (GKE) cluster with master version
1.17.14-gke.400 and node version 1.17.13-gke.2600. This is a multi-tenant cluster with hundreds of
nodes. Each node runs dozens of user workloads. Some users said DNS resolution within their Pods on
certain nodes weren&rsquo;t working. I was able to reproduce this behavior with the following steps.</p>

<p>Kubernetes schedules <code>kube-dns</code> Pods and a Service on the cluster that provide DNS and configures
kubelets to tell individual containers to use the DNS Service&rsquo;s IP to resolve DNS names. <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">See K8s
docs here</a>. First I get the <code>kube-dns</code>&lsquo; Service&rsquo;s Cluster IP. This is the IP address to
which DNS queries from Pods are sent.</p>

<pre><code>kubectl --context my-gke-cluster -n kube-system get services kube-dns
NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.178.64.10   &lt;none&gt;        53/UDP,53/TCP   666d
</code></pre>

<p>Then I make DNS queries against the Cluster IP from a Pod running on a broken node.</p>

<!-- more -->


<pre><code># Log into the GKE node
gcloud --project my-project compute ssh my-gke-node --zone us-central1-b --internal-ip

# Need to run toolbox container which has iptables command. Google's Container-Optimized OS doesn't
# have it.
dxia@my-gke-node ~ $ toolbox
20200603-00: Pulling from google-containers/toolbox
Digest: sha256:36e2f6b8aa40328453aed7917860a8dee746c101dfde4464ce173ed402c1ec57
Status: Image is up to date for gcr.io/google-containers/toolbox:20200603-00
gcr.io/google-containers/toolbox:20200603-00
e6b1ee70f91ac405623cbf1d2afa9a532a022dc644bddddd754d2cd786f58273

dxia-gcr.io_google-containers_toolbox-20200603-00
Please do not use --share-system anymore, use $SYSTEMD_NSPAWN_SHARE_* instead.
Spawning container dxia-gcr.io_google-containers_toolbox-20200603-00 on /var/lib/toolbox/dxia-gcr.io_google-containers_toolbox-20200603-00.
Press ^] three times within 1s to kill container.

# Install dig
root@toolbox:~# apt-get update &amp;&amp; apt-get install dnsutils

# Ask the kube-dns Cluster IP to resolve www.google.com
# dig will hang when it's waiting on a DNS reply. So ^C's show DNS resolution failures
root@toolbox:~# for x in $(seq 1 20); do echo ${x}; dig @10.178.64.10 www.google.com &gt; /dev/null; done
1
^C2
^C3
4
5
6
7
8
^C9
10
11
12
13
14
15
^C16
17
18
^C19
20
</code></pre>

<p>I cordoned and drained the node and added the annotation
<code>cluster-autoscaler.kubernetes.io/scale-down-disabled=true</code> to <a href="https://github.com/kubernetes/autoscaler/blob/b470c62bfa6269ed185d21d47dadc339353deb68/cluster-autoscaler/FAQ.md#how-can-i-prevent-cluster-autoscaler-from-scaling-down-a-particular-node">prevent the cluster autoscaler from
deleting it</a>.</p>

<p>Then I performed a more basic test. I tested whether I could even make a TCP connection to the
Cluster IP on port 53 (default DNS port).</p>

<pre><code># Run nc 1000 times without reverse DNS lookup, in verbose and scan mode
# Count only failed connections
root@toolbox:~# for x in $(seq 1 1000); do nc 10.178.64.10 53 -nvz 2&gt;&amp;1 | grep -v open; done | wc -l
257
</code></pre>

<p>A quarter of the TCP connections fail. This means the error is below the DNS layer at TCP layer 3.</p>

<h2>Finding the Root Cause: Down the Rabbit Hole</h2>

<p>Some background for those unfamiliar. K8s nodes (via the <code>kube-proxy</code> DaemonSet) will route IP
packets originating from a Pod with a destination of a K8s Service&rsquo;s Cluster IP to a backing Pod IP
in <a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">one of three proxy modes</a>: user space, iptables, and IPVS. I&rsquo;m assuming GKE
runs <code>kube-proxy</code> in iptables proxy mode since <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview#kube-proxy">iptables instead of IPVS is mentioned in their docs
here</a>.</p>

<p><code>kube-proxy</code> should keep the node&rsquo;s iptable rules up to date with the actual <code>kube-dns</code>
Service&rsquo;s endpoints. The following console output shows how I figured out the IP packet flow by
tracing matching iptables rules.</p>

<pre><code># List rules in FORWARD chain's filter table
root@toolbox:~# iptables -L FORWARD -t filter
Chain FORWARD (policy DROP)
target     prot opt source               destination
cali-FORWARD  all  --  anywhere             anywhere             /* cali:wUHhoiAYhphO9Mso */
KUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forwarding rules */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
DOCKER-USER  all  --  anywhere             anywhere
DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     tcp  --  anywhere             anywhere
ACCEPT     udp  --  anywhere             anywhere
ACCEPT     icmp --  anywhere             anywhere
ACCEPT     sctp --  anywhere             anywhere

# List rules in KUBE-SERVICES chain's nat table and look for rules that forward IP packets destined
# for the K8s Service kube-system/kube-dns' Cluster IP
root@toolbox:~# iptables -L KUBE-SERVICES -t nat | grep kube-system/kube-dns | grep SVC
KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  anywhere             10.178.64.10         /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:domain
KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  anywhere             10.178.64.10         /* kube-system/kube-dns:dns cluster IP */ udp dpt:domain

# List rules in KUBE-SVC-ERIFXISQEP7F7OF4 chain's nat table
Chain KUBE-SVC-ERIFXISQEP7F7OF4 (1 references)
target     prot opt source               destination
KUBE-SEP-BMNCBK7ROA3MA6UU  all  --  anywhere             anywhere             statistic mode random probability 0.01538461540
KUBE-SEP-GYUBQUCI6VR6AER2  all  --  anywhere             anywhere             statistic mode random probability 0.01562500000
KUBE-SEP-IF56RUVXN2P4ORZZ  all  --  anywhere             anywhere             statistic mode random probability 0.01587301586
KUBE-SEP-WUD7OE7TYMWFJJYX  all  --  anywhere             anywhere             statistic mode random probability 0.01612903224
KUBE-SEP-B7IYZJB6QVUX246S  all  --  anywhere             anywhere             statistic mode random probability 0.01639344264
KUBE-SEP-T6B7SPNOX3DH33BE  all  --  anywhere             anywhere             statistic mode random probability 0.01666666660
KUBE-SEP-REJSUT2VC76HMIRQ  all  --  anywhere             anywhere             statistic mode random probability 0.01694915257
KUBE-SEP-B4N4VXNUSBNXHV73  all  --  anywhere             anywhere             statistic mode random probability 0.01724137925
KUBE-SEP-XUJIW6IGZX4X5BBG  all  --  anywhere             anywhere             statistic mode random probability 0.01754385978
KUBE-SEP-MMBQBWR6AYIPMUZL  all  --  anywhere             anywhere             statistic mode random probability 0.01785714272
KUBE-SEP-6O5U6FAKQVEXGTP7  all  --  anywhere             anywhere             statistic mode random probability 0.01818181807
KUBE-SEP-DMN3RJWMPAEHNOGE  all  --  anywhere             anywhere             statistic mode random probability 0.01851851866
KUBE-SEP-FHJKZIH3JDZSXJUD  all  --  anywhere             anywhere             statistic mode random probability 0.01886792434
KUBE-SEP-YRPM7BEQS2YESSJL  all  --  anywhere             anywhere             statistic mode random probability 0.01923076902
KUBE-SEP-BSHQZGGNYIILL3V7  all  --  anywhere             anywhere             statistic mode random probability 0.01960784290
KUBE-SEP-XTW5FCAH2423EWAV  all  --  anywhere             anywhere             statistic mode random probability 0.02000000002
KUBE-SEP-2ETTGYCM3KLKL54Q  all  --  anywhere             anywhere             statistic mode random probability 0.02040816331
KUBE-SEP-ZUFFQWVT2EY73YVF  all  --  anywhere             anywhere             statistic mode random probability 0.02083333349
KUBE-SEP-VUNSBD5OILT2BGUX  all  --  anywhere             anywhere             statistic mode random probability 0.02127659554
KUBE-SEP-3XVS5OF4SBBHATZW  all  --  anywhere             anywhere             statistic mode random probability 0.02173913037
KUBE-SEP-IRW2YX5BEMBR3OGF  all  --  anywhere             anywhere             statistic mode random probability 0.02222222229
KUBE-SEP-6J6T3TOCBEQ5NUQ5  all  --  anywhere             anywhere             statistic mode random probability 0.02272727247
KUBE-SEP-E3FOMPW5DQK5FDIA  all  --  anywhere             anywhere             statistic mode random probability 0.02325581387
KUBE-SEP-EO4O2TBNDPU377YQ  all  --  anywhere             anywhere             statistic mode random probability 0.02380952379
KUBE-SEP-ZGRZOBXXZ2KPGNZD  all  --  anywhere             anywhere             statistic mode random probability 0.02439024393
KUBE-SEP-XLRCUOCE6XAL3TYE  all  --  anywhere             anywhere             statistic mode random probability 0.02499999991
KUBE-SEP-477YCBVB2RZ4WKUD  all  --  anywhere             anywhere             statistic mode random probability 0.02564102551
KUBE-SEP-FGVS22Q3OCM6S5VS  all  --  anywhere             anywhere             statistic mode random probability 0.02631578967
KUBE-SEP-FBHD55TKQKCEKSUO  all  --  anywhere             anywhere             statistic mode random probability 0.02702702722
KUBE-SEP-ULRGL5A7XKWV3HB6  all  --  anywhere             anywhere             statistic mode random probability 0.02777777798
KUBE-SEP-HO6T2NOJNNMVWDPW  all  --  anywhere             anywhere             statistic mode random probability 0.02857142873
KUBE-SEP-PV23DIU55F5LDJIX  all  --  anywhere             anywhere             statistic mode random probability 0.02941176482
KUBE-SEP-6PL2LOTBN64MN2IF  all  --  anywhere             anywhere             statistic mode random probability 0.03030303027
KUBE-SEP-3G3LTNLLVZWE57GZ  all  --  anywhere             anywhere             statistic mode random probability 0.03125000000
KUBE-SEP-SNHFF6VK2KP44I7Q  all  --  anywhere             anywhere             statistic mode random probability 0.03225806449
KUBE-SEP-KNOCRXE7JOQ4FBTI  all  --  anywhere             anywhere             statistic mode random probability 0.03333333321
KUBE-SEP-M5NXUS47V77SM3HZ  all  --  anywhere             anywhere             statistic mode random probability 0.03448275849
KUBE-SEP-VEMFKB2E3QRFFRSG  all  --  anywhere             anywhere             statistic mode random probability 0.03571428591
KUBE-SEP-RRYDQV524YXA4GDR  all  --  anywhere             anywhere             statistic mode random probability 0.03703703685
KUBE-SEP-G65AAYF5LWFW4YBM  all  --  anywhere             anywhere             statistic mode random probability 0.03846153850
KUBE-SEP-K4HN6ANXSPKA7JGZ  all  --  anywhere             anywhere             statistic mode random probability 0.04000000004
KUBE-SEP-72YXYSKWHCML6KJJ  all  --  anywhere             anywhere             statistic mode random probability 0.04166666651
KUBE-SEP-YCD5TFDQM4ELQ5WX  all  --  anywhere             anywhere             statistic mode random probability 0.04347826075
KUBE-SEP-U7N4W7N5OKDP5PNC  all  --  anywhere             anywhere             statistic mode random probability 0.04545454541
KUBE-SEP-ACPRKJJSJ73NAQNV  all  --  anywhere             anywhere             statistic mode random probability 0.04761904757
KUBE-SEP-HPAV4MFMKCM43BC2  all  --  anywhere             anywhere             statistic mode random probability 0.04999999981
KUBE-SEP-VXO5CPBPAES2GS3A  all  --  anywhere             anywhere             statistic mode random probability 0.05263157887
KUBE-SEP-LJ3HM5QDYEB4ICUB  all  --  anywhere             anywhere             statistic mode random probability 0.05555555550
KUBE-SEP-W6VORIPTN7FDPIMU  all  --  anywhere             anywhere             statistic mode random probability 0.05882352963
KUBE-SEP-A5SGQE4VKXUT2NEC  all  --  anywhere             anywhere             statistic mode random probability 0.06250000000
KUBE-SEP-4LCLRUWZUF2DDGKK  all  --  anywhere             anywhere             statistic mode random probability 0.06666666688
KUBE-SEP-K7NZ33CKVQDPMIET  all  --  anywhere             anywhere             statistic mode random probability 0.07142857136
KUBE-SEP-76ISGBIKEK2QPYDL  all  --  anywhere             anywhere             statistic mode random probability 0.07692307699
KUBE-SEP-3S5ELV7JJCII2KNO  all  --  anywhere             anywhere             statistic mode random probability 0.08333333349
KUBE-SEP-THLYLIADKU5Z5I32  all  --  anywhere             anywhere             statistic mode random probability 0.09090909082
KUBE-SEP-T7P5MBD5MAWH2XB5  all  --  anywhere             anywhere             statistic mode random probability 0.10000000009
KUBE-SEP-WQ6DVZHCVUTU5QJS  all  --  anywhere             anywhere             statistic mode random probability 0.11111111101
KUBE-SEP-5RVGOA4UDKOKKI7O  all  --  anywhere             anywhere             statistic mode random probability 0.12500000000
KUBE-SEP-VSXQV2AZ43RZQSL7  all  --  anywhere             anywhere             statistic mode random probability 0.14285714272
KUBE-SEP-RVDWX7YLRKCSUDII  all  --  anywhere             anywhere             statistic mode random probability 0.16666666651
KUBE-SEP-OECSAM56W6JQA562  all  --  anywhere             anywhere             statistic mode random probability 0.20000000019
KUBE-SEP-HY76TWODHVCVLG5Y  all  --  anywhere             anywhere             statistic mode random probability 0.25000000000
KUBE-SEP-3UNVKH34LEKZ2P5K  all  --  anywhere             anywhere             statistic mode random probability 0.33333333349
KUBE-SEP-TDCXKWGVKJJ22VHB  all  --  anywhere             anywhere             statistic mode random probability 0.50000000000
KUBE-SEP-Z7ZOTGJIY44EKMWW  all  --  anywhere             anywhere

# List the rules of two random chains above to see the DNAT'ed Pod IP
root@toolbox:~# iptables -L KUBE-SEP-RVDWX7YLRKCSUDII -t nat
Chain KUBE-SEP-RVDWX7YLRKCSUDII (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.179.94.16         anywhere
DNAT       tcp  --  anywhere             anywhere             tcp to::0 persistent:0 persistent

root@toolbox:~# iptables -L KUBE-SEP-6PL2LOTBN64MN2IF -t nat
Chain KUBE-SEP-6PL2LOTBN64MN2IF (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.179.45.66         anywhere
DNAT       tcp  --  anywhere             anywhere             tcp to::0 persistent:0 persistent
</code></pre>

<p>These final rules are the ones that actually replace the destination Cluster IP of 10.178.64.10 with
a randomly chosen <code>kube-dns</code> Pod IP. The random selection is implemented by the rules in the
<code>KUBE-SVC-ERIFXISQEP7F7OF4</code> chain which have <code>statistic mode random probability p</code>. Rules are
matched top down. So the first rule with target <code>KUBE-SEP-BMNCBK7ROA3MA6UU</code> has a probability of
0.01538461540 of being picked. The second rule with target <code>KUBE-SEP-GYUBQUCI6VR6AER2</code> has a
probability of 0.01562500000 of being picked. But this 0.01562500000 is applied to the probability
that the first rule didn&rsquo;t match. So its overall probability is (1 - 0.01538461540) * 0.01562500000
~= 0.01538461540. Applying this calculation to the other rules, you can see each rule has a
probability of 0.01538461540 or <code>1/n</code> in being selected where <code>n</code> = 65 is the total number of kube-dns
Pods in this case. This algorithm is actually a variation of <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">reservoir sampling</a>.</p>

<h3>Confirming the Root Cause</h3>

<p>At this point I strongly suspected the iptables rules were stale and routing packets to kube-dns
Pod IPs that no longer exist. In order to confirm this I wanted to find an actual DNAT&#8217;ed IP that
didn&rsquo;t correspond to any actual kube-dns Pod. There were 65 rules in the <code>KUBE-SVC-ERIFXISQEP7F7OF4</code>
chain, but I expected 77 because that was the number of <code>kube-dns</code> Pods.</p>

<pre><code>kubectl --context my-gke-cluster -n kube-system get endpoints kube-dns -o json | jq -r .subsets[0].addresses | jq length
77
</code></pre>

<p>On nodes without DNS issues, I saw the correct number of rules.</p>

<pre><code>root@healthy-gke-node:~# iptables -L KUBE-SVC-ERIFXISQEP7F7OF4 -t nat | wc -l
79 [two extra lines of headers]
</code></pre>

<p>I saw this Pod IP when inspecting a randomly chosen rule on <code>my-gke-node</code>.</p>

<pre><code>root@toolbox:~# iptables -L KUBE-SEP-RVDWX7YLRKCSUDII -t nat
Chain KUBE-SEP-RVDWX7YLRKCSUDII (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.179.94.16         anywhere
DNAT       tcp  --  anywhere             anywhere             tcp to::0 persistent:0 persistent
</code></pre>

<p>No <code>kube-dns</code> Pod existed with this IP.</p>

<pre><code>kubectl --context my-gke-cluster -n kube-system get pods --selector k8s-app=kube-dns -o wide | grep 10.179.94.16
[no output]
</code></pre>

<p>This confirmed <code>kube-proxy</code> wasn&rsquo;t updating the iptables rules for <code>kube-dns</code>. Why? The <code>kube-proxy</code>
logs on the node showed these ongoing occurring errors.</p>

<pre><code>dxia@my-gke-node ~ $ tail -f /var/log/kube-proxy.log
E0126 20:40:24.739255       1 reflector.go:153] k8s.io/client-go/informers/factory.go:135: Failed to list *v1.Service: an error on the server ("") has prevented the request from succeeding (get services)
E0126 20:40:24.739611       1 reflector.go:153] k8s.io/client-go/informers/factory.go:135: Failed to list *v1.Endpoints: an error on the server ("") has prevented the request from succeeding (get endpoints)
E0126 20:40:34.742869       1 reflector.go:153] k8s.io/client-go/informers/factory.go:135: Failed to list *v1.Service: an error on the server ("") has prevented the request from succeeding (get services)
</code></pre>

<h2>The Speculative Fix</h2>

<p>I think these <code>kube-proxy</code> errors are caused by this underlying K8s bug, but I&rsquo;m not sure.</p>

<blockquote><p>we found that after the problem occurred all subsequent requests were still send on the same
connection. It seems that although the client will resend the request to apiserver, but the
underlay http2 library still maintains the old connection so all subsequent requests are still
send on this connection and received the same error use of closed connection.</p>

<p>So the question is why http2 still maintains an already closed connection? Maybe the connection it
maintained is indeed alive but some intermediate connections are closed unexpectedly?</p></blockquote>

<p>&mdash; <a href="https://github.com/kubernetes/kubernetes/issues/87615#issuecomment-596312532">https://github.com/kubernetes/kubernetes/issues/87615#issuecomment-596312532</a></p>

<p>The bug in that issue is <a href="https://github.com/kubernetes/kubernetes/issues/87615#issuecomment-743342319">fixed in K8s 1.19 and 1.20</a>.</p>

<p>If you&rsquo;re using GKE and Google Cloud Monitoring, this log query will show which nodes&#8217; kube-proxy
Pods can&rsquo;t get updated Service and Endpoint data from the K8s API.</p>

<pre><code>resource.type="k8s_node"
resource.labels.project_id="[YOUR-PROJECT]"
logName="projects/[YOUR-PROJECT]/logs/kube-proxy"
jsonPayload.message:"Failed to list "
severity=ERROR
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Becoming a Better Public Speaker]]></title>
    <link href="https://www.davidxia.com/2019/05/becoming-a-better-public-speaker/"/>
    <updated>2019-05-26T00:04:59+02:00</updated>
    <id>https://www.davidxia.com/2019/05/becoming-a-better-public-speaker</id>
    <content type="html"><![CDATA[<p>At the beginning of this year I set a goal of becoming a better public speaker
and more visible in both tech and other broader causes I believe in. I&rsquo;m happy
to say that in the last two months I gave three talks! Two were prepared talks
with slides at tech conferences. The other was an unprepared conversation on a
podcast. These were all technical and related to my work at Spotify. Outside
of Spotify, I spoke for one minute at a mock political town hall in front of
about 30 people and at a public policy forum for ~15 minutes in front of
roughly the same number of people. But more on that later. Here are my
technical talks. These talks wouldn&rsquo;t be possible without the help, feedback,
and moral support from my Spotify colleagues.</p>

<h2>1. Keynote at KubeCon + CloudNativeCon Europe 2019 in Barcelona on May 22, 2019</h2>

<p><a href="https://kccnceu19.sched.com/event/MQbb/keynote-how-spotify-accidentally-deleted-all-its-kube-clusters-with-no-user-impact-david-xia-infrastructure-engineer-spotify">&ldquo;How Spotify Accidentally Deleted All its Kube Clusters with No User Impact&rdquo;</a></p>

<iframe width="750" height="422" src="https://www.youtube.com/embed/ix0Tw8uinWs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<h2>2. Kubernetes Podcast from Google on April 23, 2019</h2>

<p><a href="https://kubernetespodcast.com/episode/050-spotify/">&ldquo;Spotify, with David Xia&rdquo;</a>. Listen on <a href="https://open.spotify.com/show/0AsnxlMtXRUEeZkIO0ScpJ">Spotify here</a>.</p>

<h2>3. Joint talk with Google at Google Next SF on April 11, 2019</h2>

<p><a href="https://cloud.withgoogle.com/next/sf/sessions?session=HYB316">&ldquo;GKE Usage Metering: Whose Line Item Is It Anyway?&rdquo;</a></p>

<iframe width="750" height="422" src="https://www.youtube.com/embed/EuBc1v27hY8?start=1036" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My QCon NYC 2016 Slides - Reach Production Faster With Containers in Testing]]></title>
    <link href="https://www.davidxia.com/2016/06/my-qcon-nyc-2016-slides/"/>
    <updated>2016-06-20T17:15:49-04:00</updated>
    <id>https://www.davidxia.com/2016/06/my-qcon-nyc-2016-slides</id>
    <content type="html"><![CDATA[<p>Here are the slides from my <a href="https://qconnewyork.com/ny2016/presentation/reaching-production-faster-with-containers-in-testing">QCon NYC 2016 talk titled &ldquo;Reach Production Faster with Containers in
Testing.&rdquo;</a> in various formats. <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">All formats of &ldquo;Reach Production Faster with Containers in Testing&rdquo;</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://www.davidxia.com/2016/06/my-qcon-nyc-2016-slides-reach-production-faster-with-containers-in-testing/" property="cc:attributionName" rel="cc:attributionURL">David Xia</a> are licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>

<iframe src="//www.slideshare.net/slideshow/embed_code/key/p1uEfRmocwplbk" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/DavidXia/qcon-nyc-2016-reach-production-faster-with-containers-in-testing" title="QCon NYC 2016: Reach Production Faster with Containers in Testing" target="_blank">QCon NYC 2016: Reach Production Faster with Containers in Testing</a> </strong> from <strong><a href="//www.slideshare.net/DavidXia" target="_blank">David Xia</a></strong> </div></p>

<p>More formats here:</p>

<ul>
<li><a href="https://drive.google.com/file/d/0By_v8MtsRMKkSloyT3gxalRuYjQ/view?usp=sharing">PDF</a></li>
<li><a href="https://drive.google.com/file/d/0By_v8MtsRMKkSm9BVW5uWFR5TFk/view?usp=sharing">Keynote</a></li>
<li><a href="https://drive.google.com/file/d/0By_v8MtsRMKkRkpQWk5paUZOUW8/view?usp=sharing">PowerPoint</a></li>
</ul>


<p>{% img center <a href="https://i.imgur.com/w0P47Ugl.jpg">https://i.imgur.com/w0P47Ugl.jpg</a> 640 619 %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Don't Worry About Forgetting to Cancel Your Subscription - Use a Virtual Credit Card]]></title>
    <link href="https://www.davidxia.com/2016/04/dont-worry-about-forgetting-to-cancel-your-subscription-use-a-virtual-credit-card/"/>
    <updated>2016-04-21T08:55:32-04:00</updated>
    <id>https://www.davidxia.com/2016/04/dont-worry-about-forgetting-to-cancel-your-subscription-use-a-virtual-credit-card</id>
    <content type="html"><![CDATA[<p>Almost all of us have signed up for a free trial we then forgot to cancel
before getting charged for the next month or have felt uneasy entering credit card information
on a sketchy website. I discovered virtual credit card numbers several months ago and now use them
for both these cases.</p>

<p>Virtual credit card numbers (VCN) are credit card numbers that aren&rsquo;t associated with a physical
card. They are usually tied to your regular credit card account, and charges to the VCN will show
up on that credit card statement.</p>

<p>VCNs have two great features on top of regular credit card numbers. <strong>You can set the expiration and
maximum limit can be charged, and you can cancel them at any time.</strong> Let&rsquo;s say I want to try
<a href="https://www.spotify.com/premium/">Spotify Premium</a>. It&rsquo;s free for a month and $10 per month afterwards. I generate a VCN, set the limit
to $1 with an expiration of one month, enter that number into Spotify&rsquo;s website, and forget about
it. Use a VCN when you&rsquo;re wary that an online merchant might steal or is unable to protect your
card info as well.</p>

<p>So how does one get a VCN? The only free and bank-sponsored credit cards I know of that offer
them are <a href="https://www.bankofamerica.com/privacy/accounts-cards/shopsafe.go">Bank of America&rsquo;s ShopSafe feature</a> and <a href="https://www.cardbenefits.citi.com/products/virtual-account-numbers.aspx">Citibank&rsquo;s Virtual Account numbers</a>.
(<a href="https://www.quora.com/Why-did-American-Express-discontinue-their-one-time-use-virtual-credit-card-numbers">Amex seems to have discontinued theirs.</a>)</p>

<p>There are cases when using a VCN might create more hassle than it&rsquo;s worth. The two I can think of
are booking a hotel room and needing to show your card when you arrive in person and returning a
purchase and trying to get a refund.</p>
]]></content>
  </entry>
  
</feed>
