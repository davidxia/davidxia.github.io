<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming & Software | David Xia]]></title>
  <link href="https://www.davidxia.com/categories/programming-and-software/atom.xml" rel="self"/>
  <link href="https://www.davidxia.com/"/>
  <updated>2020-04-01T12:34:35-04:00</updated>
  <id>https://www.davidxia.com/</id>
  <author>
    <name><![CDATA[David Xia]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[3 Levels of Load Testing GKE Workload Identity]]></title>
    <link href="https://www.davidxia.com/2020/04/3-levels-of-load-testing-gke-workload-identity/"/>
    <updated>2020-04-01T10:21:11-04:00</updated>
    <id>https://www.davidxia.com/2020/04/3-levels-of-load-testing-gke-workload-identity</id>
    <content type="html"><![CDATA[<p>I manage multitenant <a href="https://cloud.google.com/kubernetes-engine">Google Kubernetes Engine</a> (GKE) clusters for stateless backend services at
work. Google recently graduated GKE&rsquo;s <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity</a> (WI) feature to generally available
(GA). When my team used WI during its beta stage, it seemed to fail when there were more than 16
requests per second (RPS) on one GKE node to retrieve Google access tokens.</p>

<p>Before we knew about this low RPS failure threshold, we told many internal Spotify engineering teams
to go ahead and use the feature. In hindsight, we should&rsquo;ve load-tested the feature before making it
generally available internally especially since it wasn&rsquo;t even GA publicly.</p>

<p>My efforts to load test WI have grown more sophisticated over time. This post describes the
progression. It&rsquo;s like the &ldquo;4 Levels of &hellip;&rdquo; <a href="https://www.youtube.com/playlist?list=PLz3-p2q6vFYUDvVUu_aPhGUV-3ROIa6d2">Epicurious Youtube videos</a>. The goal here is to find
out at what RPS WI starts to fail and to try to learn some generalizable lessons from load testing
vendor-managed services.</p>

<h3>tl;dr lessons learned</h3>

<ul>
<li>always load test new features above and beyond what you expect your production load will be</li>
<li>use proper load testing tools and not bash for loops</li>
</ul>


<h2>My specific GKE cluster configuration</h2>

<ul>
<li>GKE masters and nodes running version 1.15.9-gke.22</li>
<li>regional cluster in Google Cloud Platform (GCP) (not on-premise)</li>
<li>4 GKE nodes that are n1-standard-32 GCE instances in one node pool</li>
<li>each node is configured to have a maximum of 32 Pods</li>
<li>cluster and node pool have WI enabled</li>
</ul>


<h2>High level of what Workload Identity is and how it works</h2>

<p>Workloads on GKE often need to access GCP resources like PubSub or CloudSQL.
In order to do so, your workload needs to use a Google Service Account (GSA) key that is authorized to
access those resources. So you end up creating keys for all your GSA&rsquo;s and copy-pasting
these keys into Kubernetes Secrets for your workloads. This is insecure and not maintainable if you
are a company that has dozens of engineering teams and hundreds of workloads.</p>

<p>So GCP offered WI which allows a Kubernetes Service Account (KSA) to be associated with a GSA. If a
workload can run with a certain KSA, it&rsquo;ll transparently get the Google access token for the
associated GSA. No manual copy-pasting GSA keys!</p>

<p>How does this work? You have to enable WI on your cluster and node pool. This creates a
<code>gke-metadata-server</code> DaemonSet in the <code>kube-system</code> namespace. <code>gke-metadata-server</code> is the
entrypoint to the whole WI system. Here&rsquo;s a nice <a href="https://www.youtube.com/watch?v=s4NYEJDFc0M">Google Cloud Next conference talk</a> with more
details.</p>

<p><code>gke-metadata-server</code> is the only part of WI that is exposed to GKE users, i.e. runs on machines you
control. It&rsquo;s like the Verizon FiOS box in your basement. You control your house, but there&rsquo;s a
little box that Verizon owns and operates in there. All other parts of WI run on GCP infrastructure
that you can&rsquo;t see. When I saw failures with WI, it all seemed to happen in
<code>gke-metadata-server</code>. So that&rsquo;s what I&rsquo;ll load test.</p>

<p>Here&rsquo;s the <code>gke-metadata-server</code> DaemonSet YAML for reference. As of the time of this writing the
image is <code>gke.gcr.io/gke-metadata-server:20200218_1145_RC0</code>. You might see different behavior with
different images.</p>

<pre><code>apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  creationTimestamp: "2019-10-15T17:04:40Z"
  generation: 8
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: gke-metadata-server
  name: gke-metadata-server
  namespace: kube-system
  resourceVersion: "138588210"
  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/daemonsets/gke-metadata-server
  uid: e06885d8-ef6d-11e9-88c9-42010a8c0110
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: gke-metadata-server
  template:
    metadata:
      annotations:
        components.gke.io/component-name: gke-metadata-server
        components.gke.io/component-version: 0.2.21
        scheduler.alpha.kubernetes.io/critical-pod: '"''"'
      creationTimestamp: null
      labels:
        addonmanager.kubernetes.io/mode: Reconcile
        k8s-app: gke-metadata-server
    spec:
      containers:
      - command:
        - /gke-metadata-server
        - --logtostderr
        - --token-exchange-endpoint=https://securetoken.googleapis.com/v1/identitybindingtoken
        - --identity-namespace=[redacted].svc.id.goog
        - --identity-provider-id=https://container.googleapis.com/v1/projects/[redacted]/locations/asia-east1/clusters/[redacted]
        - --passthrough-ksa-list=kube-system:container-watcher-pod-reader,kube-system:event-exporter-sa,kube-system:fluentd-gcp-scaler,kube-system:heapster,kube-system:kube-dns,kube-system:metadata-agent,kube-system:network-metering-agent,kube-system:securityprofile-controller,istio-system:istio-ingressgateway-service-account,istio-system:cluster-local-gateway-service-account,csm:csm-sync-agent,knative-serving:controller
        - --attributes=cluster-name=[redacted],cluster-uid=[redacted],cluster-location=asia-east1
        - --enable-identity-endpoint=true
        - --cluster-uid=[redacted]
        image: gke.gcr.io/gke-metadata-server:20200218_1145_RC0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 54898
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: gke-metadata-server
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
          requests:
            cpu: 100m
            memory: 100Mi
        securityContext:
          privileged: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/kubelet/kubeconfig
          name: kubelet-credentials
          readOnly: true
        - mountPath: /var/lib/kubelet/pki/
          name: kubelet-certs
          readOnly: true
        - mountPath: /var/run/
          name: container-runtime-interface
        - mountPath: /etc/srv/kubernetes/pki
          name: kubelet-pki
          readOnly: true
        - mountPath: /etc/ssl/certs/
          name: ca-certificates
          readOnly: true
      dnsPolicy: Default
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/os: linux
        iam.gke.io/gke-metadata-server-enabled: "true"
      priorityClassName: system-node-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: gke-metadata-server
      serviceAccountName: gke-metadata-server
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        operator: Exists
      - effect: NoSchedule
        operator: Exists
      volumes:
      - hostPath:
          path: /var/lib/kubelet/pki/
          type: Directory
        name: kubelet-certs
      - hostPath:
          path: /var/lib/kubelet/kubeconfig
          type: File
        name: kubelet-credentials
      - hostPath:
          path: /var/run/
          type: Directory
        name: container-runtime-interface
      - hostPath:
          path: /etc/srv/kubernetes/pki/
          type: Directory
        name: kubelet-pki
      - hostPath:
          path: /etc/ssl/certs/
          type: Directory
        name: ca-certificates
  templateGeneration: 8
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
</code></pre>

<h2>Level 1</h2>

<p>What kind of load am I putting on <code>gke-metadata-server</code>? Since this DaemonSet exists to give out
Google access tokens, I&rsquo;ll send it HTTP requests asking for such tokens.</p>

<p>I built a Docker image with the following <code>Dockerfile</code>.</p>

<pre><code>FROM google/cloud-sdk
ENTRYPOINT while true; do for i in {1..20}; do curl -X GET https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=$(gcloud auth print-access-token) &amp; done; wait; done;
</code></pre>

<p>Then I created the following K8s Deployment YAML.</p>

<pre><code class="yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: wi-test
  namespace: [K8S_NAMESPACE]
spec:
  replicas: 7
  selector:
    matchLabels:
      app: wi-test
  template:
    metadata:
      labels:
        app: wi-test
    spec:
      nodeSelector:
        kubernetes.io/hostname: [NODE-NAME]
      containers:
      - image: my-docker-image
        name: workload-identity-test
</code></pre>

<p>I ran seven of these Pods on a single node (see the <code>nodeSelector</code> above) to target a single
instance of <code>gke-metadata-server</code>.</p>

<p>This isn&rsquo;t a great test because there&rsquo;s a lot of extra work performed by the Container in running
<code>gcloud</code> to print a Google access token (there may be bottlenecks in this <code>gcloud</code> command itself
which is Python code), curling the <code>googleapis.com</code> endpoint to get the token info (originally done
to verify the token was valid). And there&rsquo;s probably bottlenecks in using a shell to do this. All in
all, this implementation doesn&rsquo;t really let you specify a fixed RPS. You&rsquo;re at the mercy of how fast
your Container, shell, gcloud, and the network will let you execute this. I also wasn&rsquo;t able to run
more Pods on a single node because I was hitting the max 32 pods per node limit. There were already
a bunch of other GKE-system level workloads like Calico that took up node capacity.</p>

<h2>Level 2</h2>

<p>Apply this one Pod</p>

<pre><code>cat &lt;&lt;EOF | kubectl --context [CONTEXT] apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: wi-test
  namespace: [K8S_NAMESPACE]
spec:
  containers:
  - image: google/cloud-sdk
    name: wi-test
    command: [ '/bin/bash', '-c', '--' ]
    args: [ 'while true; do sleep 30; done;' ]
    securityContext:
      allowPrivilegeEscalation: false
      privileged: false
      readOnlyRootFilesystem: false
    resources:
      limits:
        cpu: 2
        memory: 4G
      requests:
        cpu: 2
        memory: 4G
EOF
</code></pre>

<p>Then <code>kubectl exec</code> in and run this command.</p>

<pre><code>for i in {1..N}; do gcloud auth print-access-token &amp; done; wait;
</code></pre>

<p>Everything seemed to work fine when N was 100. When N was 200 I got a few errors like the below.
They look like client-side errors and not server ones though.</p>

<pre><code>ERROR: gcloud failed to load: No module named 'ruamel.yaml.error'
gcloud_main = _import_gcloud_main()
import googlecloudsdk.gcloud_main
from googlecloudsdk.api_lib.iamcredentials import util as iamcred_util
from googlecloudsdk.api_lib.util import exceptions
from googlecloudsdk.core.resource import resource_printer
from googlecloudsdk.core.resource import yaml_printer
from googlecloudsdk.core.yaml import dict_like
from googlecloudsdk.core import yaml_location_value
from ruamel import yaml
from ruamel.yaml.main import * # NOQA
from ruamel.yaml.error import UnsafeLoaderWarning, YAMLError # NOQA

This usually indicates corruption in your gcloud installation or problems with your Python interpreter.

Please verify that the following is the path to a working Python 2.7 or 3.5+ executable:
/usr/bin/python3

If it is not, please set the CLOUDSDK_PYTHON environment variable to point to a working Python 2.7 or 3.5+ executable.

If you are still experiencing problems, please reinstall the Cloud SDK using the instructions here:
https://cloud.google.com/sdk/

ERROR: gcloud failed to load: cannot import name 'opentype' from 'pyasn1.type' (/usr/bin/../lib/google-cloud-sdk/lib/third_party/pyasn1/type/__init__.py)
from google.auth.crypt import _cryptography_rsa
import cryptography.exceptions


File "/usr/bin/../lib/google-cloud-sdk/lib/gcloud.py", line 67, in main
File "/usr/bin/../lib/google-cloud-sdk/lib/gcloud.py", line 48, in _import_gcloud_main
File "/usr/lib/google-cloud-sdk/lib/googlecloudsdk/gcloud_main.py", line 33, in &lt;module&gt;
File "/usr/lib/google-cloud-sdk/lib/googlecloudsdk/api_lib
</code></pre>

<p><code>gcloud</code> does not synchronize between processes with concurrent invokations. It sometimes writes
files to disk. So this is also not a great load test because it still doesn&rsquo;t let you achieve a
specific RPS and has client-side bottlenecks.</p>

<h2>Level 3</h2>

<p>Use a proper HTTP load testing tool. A colleague told me about <a href="https://github.com/tsenart/vegeta"><code>vegeta</code></a>.
It&rsquo;s a seemingly good tool, but, more importantly, its commands are amazing.
<code>vegeta attack ...</code>.</p>

<p>I first start a <code>golang</code> Pod that just busy-waits.</p>

<pre><code>$ cat &lt;&lt;EOF | kubectl --context [CONTEXT] apply -f -
&gt; apiVersion: v1
&gt; kind: Pod
&gt; metadata:
&gt;   name: wi-test
&gt;   namespace: [NAMESPACE]
&gt; spec:
&gt;   containers:
&gt;   - image: golang:latest
&gt;     name: wi-test
&gt;     command: [ '/bin/bash', '-c', '--' ]
&gt;     args: [ 'while true; do sleep 30; done;' ]
&gt;     resources:
&gt;       limits:
&gt;         cpu: 2
&gt;         memory: 4G
&gt;       requests:
&gt;         cpu: 2
&gt;         memory: 4G
&gt; EOF

pod/wi-test created
</code></pre>

<p>Then I get a shell in it.</p>

<pre><code>kubectl --context [CONTEXT] -n [NAMESPACE] exec -it wi-test bash

Defaulting container name to wi-test.
Use 'kubectl describe pod/wi-test -n [NAMESPACE]' to see all of the containers in this pod.

root@wi-test:/go# go get github.com/tsenart/vegeta
root@wi-test:/go# vegeta -version

Version:
Commit:
Runtime: go1.14.1 linux/amd64
Date:
</code></pre>

<p>Let&rsquo;s throw some load on WI! <code>my-gsa@my-project.iam.gserviceaccount.com</code> is the GSA associated with
the KSA your workload runs as.</p>

<pre><code>root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 10 -duration=5s | vegeta report

Requests      [total, rate, throughput]         50, 10.20, 10.20
Duration      [total, attack, wait]             4.904s, 4.9s, 4.168ms
Latencies     [min, mean, 50, 90, 95, 99, max]  4.168ms, 6.137ms, 5.039ms, 9.591ms, 10.444ms, 31.452ms, 31.452ms
Bytes In      [total, mean]                     25300, 506.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:50
Error Set:

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 1000 -duration=5s | vegeta report
Requests      [total, rate, throughput]         5000, 1000.20, 127.51
Duration      [total, attack, wait]             31.175s, 4.999s, 26.176s
Latencies     [min, mean, 50, 90, 95, 99, max]  101.972ms, 11.003s, 7.652s, 30s, 30s, 30s, 30.001s
Bytes In      [total, mean]                     2011350, 402.27
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           79.50%
Status Codes  [code:count]                      0:1025  200:3975
Error Set:
Get "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token": context deadline exceeded (Client.Timeout exceeded while awaiting headers)

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 100 -duration=5s | vegeta report
Requests      [total, rate, throughput]         500, 100.20, 98.40
Duration      [total, attack, wait]             5.081s, 4.99s, 91.244ms
Latencies     [min, mean, 50, 90, 95, 99, max]  3.805ms, 106.449ms, 59.058ms, 306.334ms, 372.519ms, 506.703ms, 601.534ms
Bytes In      [total, mean]                     253000, 506.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:500
Error Set:

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 500 -duration=5s | vegeta report
Requests      [total, rate, throughput]         2500, 500.20, 43.29
Duration      [total, attack, wait]             34.072s, 4.998s, 29.074s
Latencies     [min, mean, 50, 90, 95, 99, max]  10.56ms, 12.579s, 756.03ms, 30s, 30s, 30s, 30.001s
Bytes In      [total, mean]                     746350, 298.54
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           59.00%
Status Codes  [code:count]                      0:1025  200:1475
Error Set:
Get "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token": context deadline exceeded (Client.Timeout exceeded while awaiting headers)

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 250 -duration=5s | vegeta report
Requests      [total, rate, throughput]         1250, 250.22, 28.52
Duration      [total, attack, wait]             34.996s, 4.996s, 30s
Latencies     [min, mean, 50, 90, 95, 99, max]  8.331ms, 6.347s, 376.419ms, 30s, 30s, 30s, 30.001s
Bytes In      [total, mean]                     504988, 403.99
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           79.84%
Status Codes  [code:count]                      0:252  200:998
Error Set:
Get "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token": context deadline exceeded (Client.Timeout exceeded while awaiting headers)

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 200 -duration=5s | vegeta report
Requests      [total, rate, throughput]         1000, 200.20, 28.28
Duration      [total, attack, wait]             32.43s, 4.995s, 27.435s
Latencies     [min, mean, 50, 90, 95, 99, max]  9.985ms, 2.739s, 188.509ms, 797.058ms, 30s, 30s, 30s
Bytes In      [total, mean]                     464002, 464.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           91.70%
Status Codes  [code:count]                      0:83  200:917
Error Set:
Get "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token": context deadline exceeded (Client.Timeout exceeded while awaiting headers)

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 150 -duration=5s | vegeta report
Requests      [total, rate, throughput]         750, 150.20, 146.53
Duration      [total, attack, wait]             5.118s, 4.993s, 125.078ms
Latencies     [min, mean, 50, 90, 95, 99, max]  3.747ms, 224.285ms, 171.325ms, 460.236ms, 549.18ms, 682.161ms, 892.25ms
Bytes In      [total, mean]                     379500, 506.00
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           100.00%
Status Codes  [code:count]                      200:750
Error Set:

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 175 -duration=5s | vegeta report
Requests      [total, rate, throughput]         875, 175.20, 24.46
Duration      [total, attack, wait]             34.097s, 4.994s, 29.103s
Latencies     [min, mean, 50, 90, 95, 99, max]  3.704ms, 1.687s, 231.652ms, 708.672ms, 2.432s, 30s, 30s
Bytes In      [total, mean]                     422004, 482.29
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           95.31%
Status Codes  [code:count]                      0:41  200:834
Error Set:
Get "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token": context deadline exceeded (Client.Timeout exceeded while awaiting headers)

root@wi-test:/go# echo "GET http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token" | vegeta attack -header 'Metadata-Flavor: Google' -rate 165 -duration=5s | vegeta report
Requests      [total, rate, throughput]         825, 165.20, 23.61
Duration      [total, attack, wait]             34.6s, 4.994s, 29.606s
Latencies     [min, mean, 50, 90, 95, 99, max]  3.483ms, 558.613ms, 222.111ms, 531.49ms, 622.473ms, 11.851s, 30s
Bytes In      [total, mean]                     413402, 501.09
Bytes Out     [total, mean]                     0, 0.00
Success       [ratio]                           99.03%
Status Codes  [code:count]                      0:8  200:817
Error Set:
Get "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/my-gsa@my-project.iam.gserviceaccount.com/token": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
</code></pre>

<p>After more bisection, I found that this specific instance of <code>gke-metadata-server</code>
starts to fail around 150RPS. When it does fail, p99 latency skyrockets from less than 1 second to
30 seconds. This is usually a sign of a rate limiter or quota.</p>

<p>How have you tried load testing WI or other GKE features? What&rsquo;re your favorite load testing tools
for these cases, and what interesting behavior have you found?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[More About Nginx DNS Resolution Than You Ever Wanted to Know]]></title>
    <link href="https://www.davidxia.com/2019/05/more-about-nginx-dns-resolution-than-you-ever-wanted-to-know/"/>
    <updated>2019-05-17T12:58:38-04:00</updated>
    <id>https://www.davidxia.com/2019/05/more-about-nginx-dns-resolution-than-you-ever-wanted-to-know</id>
    <content type="html"><![CDATA[<p>This is a post about Nginx&rsquo;s DNS resolution behavior I didn&rsquo;t know about but wish I did before I
started using Kubernetes (K8s).</p>

<h2>Nginx caches statically configured domains once</h2>

<h3>Symptoms</h3>

<p>I moved a backend service <code>foo</code> from running on a virtual
machine to K8s. Foo&rsquo;s clients include an Nginx instance configured with this <code>upstream</code>
block.</p>

<pre><code>upstream foo {
  server foo.example.com.;
}

server {
  ...

  location ~* /_foo/(.*) {
    proxy_pass https://foo/$1;
    ...
  }
}
</code></pre>

<p>K8s Pods can be rescheduled anytime so their IPs aren&rsquo;t stable. I&rsquo;m supposed to use K8s Services
to avoid caching these ephemeral Pod IPs. But in my case because of interoperability reasons I was
registering Pod IPs directly as A records for <code>foo.example.com.</code>. I started noticing that after my Pod
IPs changed either because of rescheduling or updating the Deployment, Nginx started throwing
<code>502 Bad Gateway</code> errors.</p>

<h3>Root Problem</h3>

<p>Nginx resolves statically configured domain names only once at startup or configuration
reload time. So Nginx resolved <code>foo.example.com.</code> once at startup to several Pod IPs and cached
them forever.</p>

<h3>Solution</h3>

<!-- more -->


<p>Using a variable for the domain name will make Nginx resolve and cache it using the TTL value of the
DNS response. So replace the <code>upstream</code> block with a variable. I have no idea why it has to be a
variable to make Nginx resolve the domain periodically.</p>

<pre><code>set $foo_url foo.example.com.;
</code></pre>

<p>And replace the <code>proxy_pass</code> line with</p>

<pre><code>  location ~* /_foo/(.*) {
    proxy_pass https://$foo_url/$1;
    ...
  }
</code></pre>

<p>This behavior isn&rsquo;t documented but has been observed empirically and discussed <a href="https://serverfault.com/a/593003/88755">here</a>, <a href="https://stackoverflow.com/a/41476524/553994">here</a>,
and <a href="https://www.ruby-forum.com/t/using-127-0-0-1-in-resolver/238180/4">here</a>. I also learned that this setup requires me to define a <code>resolver</code> in the Nginx configs.
<strong>For some reason Nginx resolves statically configured domains by querying the nameserver specified in
<code>/etc/resolv.conf</code> but periodically resolved domains require a completely different config
setting.</strong> I would love to know why.</p>

<p>The VM on which Nginx was running ran a Bind DNS server locally, so I set <code>resolver 127.0.0.1</code>.
I triggered the code path that made Nginx send requests to foo and saw periodic DNS queries
occurring with <code>sudo tcpdump -i lo -n dst port 53 | grep foo</code>.</p>

<h2>What if that Nginx is also running on K8s?</h2>

<h3>Problem</h3>

<p>I had another Nginx instance that also made requests to foo. This Nginx was running on K8s too. It
was created with this Deployment YAML.</p>

<pre><code class="yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: openresty/openresty:trusty
        ports:
          - name: https
            containerPort: 443
            protocol: TCP
        volumeMounts:
          - name: nginx-config
            mountPath: /etc/nginx/conf.d
      volumes:
        - name: nginx-config
          configMap:
            name: nginx-config
</code></pre>

<p>The <code>nginx-config</code> ConfigMap was</p>

<pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    upstream foo {
      server foo.example.com.:443;
    }

    server {
      ...

      # use regex capture to preserve url path and query params
      location ~* /_foo/(.*) {
        proxy_pass https://foo/$1;
        ...
      }
    }
</code></pre>

<p>I replaced <code>upstream</code> with the same pattern above, but in this case when I needed to define
<code>resolver</code> I couldn&rsquo;t use <code>127.0.0.1</code> because there&rsquo;s no Bind running locally. I can&rsquo;t hardcode the
resolver because it might change.</p>

<h3>Solution: run Nginx and foo on the same K8s cluster and use the cluster-local Service DNS record</h3>

<p>If Nginx and foo run on the same K8s cluster, I can use the cluster-local DNS record created by a
K8s Service matching the foo Pods. A Service like this</p>

<pre><code class="yaml">apiVersion: v1
kind: Service
metadata:
  name: foo
  namespace: bar
...
</code></pre>

<p>will create a DNS A record <code>foo.bar.svc.cluster.local.</code> pointing to the K8s Service&rsquo;s IP.
Since this Service&rsquo;s IP is stable and it load balances requests to the underlying Pods, there&rsquo;s no need for Nginx to
periodically lookup the Pod IPs. I can keep the <code>upstream</code> block like so.</p>

<pre><code>upstream foo {
  server foo.bar.svc.cluster.local.:443;
}
</code></pre>

<p>As its name implies, <code>foo.bar.svc.cluster.local.</code> is only resolvable within the cluster. So
Nginx has to be running on the same cluster as foo.</p>

<h3>Solution: dynamically set the Nginx <code>resolver</code> equal to the system&rsquo;s when the Pod starts</h3>

<p>What if Nginx is on another K8s cluster? Then I can set <code>resolver</code> to the IP of one of the
nameservers in <code>/etc/resolv.conf</code>. After a bunch of tinkering I came up with this way to dynamically
set the Nginx <code>resolver</code> when the Pod starts. A placeholder for <code>resolver</code> is set in the Nginx
ConfigMap, and a command at Pod startup copies over the templated config and replaces the
placeholder with a nameserver IP from <code>/etc/resolv.conf</code>.</p>

<p>Change <code>nginx-config</code> ConfigMap to</p>

<pre><code class="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf.template: |
    server {
      ...

      # This directive is dynamic because we set it to the
      # kube-dns Service IP which is different for each cluster.
      resolver $NAMESERVER;

      set $foo_url foo.example.com.;

      # use regex capture to preserve url path and query params
      location ~* /_foo/(.*) {
        proxy_pass https://$foo_url/$1;
        ...
      }
    }
</code></pre>

<p>Deployment YAML then becomes (note the added <code>command</code>, <code>args</code>, and new <code>volume</code> and <code>volumeMount</code>).</p>

<pre><code class="yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: openresty/openresty:trusty
        command: ['/bin/bash', '-c']
        args:
        - |
          export NAMESERVER=$(grep 'nameserver' /etc/resolv.conf | awk '{print $2}' | tr '\n' ' ')
          echo "Nameserver is: $NAMESERVER"
          echo 'Copying nginx config'
          envsubst '$NAMESERVER' &lt; /etc/nginx/conf.d.template/nginx.conf.template &gt; /etc/nginx/conf.d/nginx.conf
          echo 'Using nginx config:'
          cat /etc/nginx/conf.d/nginx.conf
          echo 'Starting nginx'
          nginx -g 'daemon off;'
        ports:
          - name: https
            containerPort: 443
            protocol: TCP
        volumeMounts:
          - name: nginx-config-template
            mountPath: /etc/nginx/conf.d.template
          - name: nginx-config
            mountPath: /etc/nginx/conf.d
      volumes:
        - name: nginx-config
          emptyDir: {}
        - name: nginx-config-template
          configMap:
            name: nginx-config
</code></pre>

<p>A <code>volume</code> of type <code>emptyDir</code> is needed because recent versions of K8s made configMap volumes
read-only. EmptyDir types are writable.</p>

<p>Hopefully this helps some people out there who don&rsquo;t want to spend as much time as I did Googling
obscure Nginx behavior.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Upstream Kubernetes Java Models Is Much Better Than Raw YAML]]></title>
    <link href="https://www.davidxia.com/2018/11/using-upstream-kubernetes-java-models-is-much-better-than-raw-yaml/"/>
    <updated>2018-11-11T17:11:38-05:00</updated>
    <id>https://www.davidxia.com/2018/11/using-upstream-kubernetes-java-models-is-much-better-than-raw-yaml</id>
    <content type="html"><![CDATA[<p>It&rsquo;s been a while since I blogged about something tech related, but I had some
free time today.</p>

<p>Recently, I&rsquo;ve been trying to refactor an internal Spotify deployment tool my
team built and maintains. This deployment tool takes Kubernetes (k8s) YAML
manifests, changes them, and essentially runs <code>kubectl apply</code>. We add metadata
to the k8s manifests like labels.</p>

<p>Right now this tool receives the input YAML as strings, converts them to
Jackson ObjectNodes, and manipulates those ObjectNodes. The disadvantage of
this is that there&rsquo;s no k8s type-safety. We might accidentally add a field to a
Deployment that isn&rsquo;t valid or remove something from a Service that&rsquo;s required.</p>

<p>My refactor uses upstream k8s model classes from <a href="https://github.com/kubernetes-client/java">kubernetes-client/java</a>
which are themselves <a href="https://github.com/kubernetes-client/gen">generated</a> from the official Swagger spec. Here&rsquo;s a
helpful Yaml utility class that deserializes YAML strings into concrete classes
and can also serialize them back into YAML strings. So helpful.</p>

<p>Unfortunately, there&rsquo;s some bugs in the YAML (de)serialization that prevent me
from finishing this effort.</p>

<ul>
<li><a href="https://github.com/kubernetes-client/java/pull/417">kubernetes-client/java issue 417</a></li>
<li><a href="https://github.com/kubernetes-client/java/issues/431">kubernetes-client/java issue 431</a></li>
<li><a href="https://github.com/kubernetes-client/java/issues/340">kubernetes-client/java issue 340</a></li>
</ul>


<p>Nonetheless, it&rsquo;ll be much nicer to change k8s resources in a type-safe way
instead of parsing and rewriting raw YAML strings.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Useful Site for TLS Server Test]]></title>
    <link href="https://www.davidxia.com/2017/05/useful-site-for-testing-tls-server-configuration/"/>
    <updated>2017-05-20T12:49:20-04:00</updated>
    <id>https://www.davidxia.com/2017/05/useful-site-for-testing-tls-server-configuration</id>
    <content type="html"><![CDATA[<p>My home server&rsquo;s hard disk&rsquo;s partition map was somehow corrupted. So I&rsquo;m serving this website
from <a href="https://m.do.co/c/74c553045962">Digital Ocean</a> for now instead of my apartment. While rewriting the nginx server configs,
I found this useful site that <a href="https://www.ssllabs.com/ssltest/index.html">tests your server&rsquo;s TLS configuration</a>. It&rsquo;ll give you a grade
and warn you of weak encryption, key exchange protocols, cipher suites, etc.</p>

<p><a href="https://mozilla.github.io/server-side-tls/ssl-config-generator/">Mozilla&rsquo;s TLS configuration generator</a> is useful for providing secure defaults.</p>

<p>I&rsquo;m proud to say this site has an A.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My QCon NYC 2016 Slides - Reach Production Faster With Containers in Testing]]></title>
    <link href="https://www.davidxia.com/2016/06/my-qcon-nyc-2016-slides/"/>
    <updated>2016-06-20T17:15:49-04:00</updated>
    <id>https://www.davidxia.com/2016/06/my-qcon-nyc-2016-slides</id>
    <content type="html"><![CDATA[<p>Here are the slides from my <a href="https://qconnewyork.com/ny2016/presentation/reaching-production-faster-with-containers-in-testing">QCon NYC 2016 talk titled &ldquo;Reach Production Faster with Containers in
Testing.&rdquo;</a> in various formats. <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">All formats of &ldquo;Reach Production Faster with Containers in Testing&rdquo;</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://www.davidxia.com/2016/06/my-qcon-nyc-2016-slides-reach-production-faster-with-containers-in-testing/" property="cc:attributionName" rel="cc:attributionURL">David Xia</a> are licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>

<iframe src="//www.slideshare.net/slideshow/embed_code/key/p1uEfRmocwplbk" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/DavidXia/qcon-nyc-2016-reach-production-faster-with-containers-in-testing" title="QCon NYC 2016: Reach Production Faster with Containers in Testing" target="_blank">QCon NYC 2016: Reach Production Faster with Containers in Testing</a> </strong> from <strong><a href="//www.slideshare.net/DavidXia" target="_blank">David Xia</a></strong> </div></p>

<p>More formats here:</p>

<ul>
<li><a href="https://drive.google.com/file/d/0By_v8MtsRMKkSloyT3gxalRuYjQ/view?usp=sharing">PDF</a></li>
<li><a href="https://drive.google.com/file/d/0By_v8MtsRMKkSm9BVW5uWFR5TFk/view?usp=sharing">Keynote</a></li>
<li><a href="https://drive.google.com/file/d/0By_v8MtsRMKkRkpQWk5paUZOUW8/view?usp=sharing">PowerPoint</a></li>
</ul>


<p><img class="center <a" src="href="https://i.imgur.com/w0P47Ugl.jpg">https://i.imgur.com/w0P47Ugl.jpg</a>&#8221; width=&#8221;640&#8221; height=&#8221;619&#8221;></p>
]]></content>
  </entry>
  
</feed>
